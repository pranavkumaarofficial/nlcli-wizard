{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning Gemma 3 1B for CLI Command Translation\n",
        "\n",
        "This notebook trains a 1B parameter model to translate natural language\n",
        "to CLI commands. Runs on free Colab T4 GPU in ~2.5 hours.\n",
        "\n",
        "**What you'll build:**\n",
        "- 80-90% accuracy command translator\n",
        "- ~800MB quantized model\n",
        "- Runs locally on CPU (~1.5s inference)\n",
        "\n",
        "**No ML experience required** - all steps explained.\n",
        "\"\"\"\n",
        "\n",
        "# Cell 2: Setup (auto-installs)\n",
        "!pip install unsloth transformers datasets -q\n",
        "\n",
        "# Cell 3: Clone repo\n",
        "!git clone https://github.com/pranavkumaarofficial/nlcli-wizard"
      ],
      "metadata": {
        "id": "T5i8Vl-VjoeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pranavkumaarofficial/nlcli-wizard.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4mKkG0NOSbS",
        "outputId": "6c34ee04-59ba-409a-8834-b3f5fd686494"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nlcli-wizard'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 134 (delta 6), reused 23 (delta 4), pack-reused 101 (from 1)\u001b[K\n",
            "Receiving objects: 100% (134/134), 77.46 MiB | 13.21 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Encountered 1 file(s) that should have been pointers, but weren't:\n",
            "\tmodels/readme.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Go to your cloned repo directory first\n",
        "%cd /content/nlcli-wizard\n",
        "\n",
        "# Pull the latest changes from the remote branch\n",
        "!git pull origin main -r --autostash\n",
        "!apt-get install git-lfs -y\n",
        "!git lfs install\n",
        "!git lfs pull\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCQB4DE2OO1w",
        "outputId": "70853e25-c257-4a82-999f-70a88bc4c1e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nlcli-wizard\n",
            "From https://github.com/pranavkumaarofficial/nlcli-wizard\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "CUDA available: True\n",
            "Device: cuda\n",
            "GPU name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZy3lRdgOEux"
      },
      "source": [
        "# Fine-tuning Gemma 3 1B for venvy CLI Translation\n",
        "\n",
        "REPO: https://github.com/pranavkumaarofficial/nlcli-wizard\n",
        "\n",
        "**Project**: nlcli-wizard  \n",
        "**Model**: google/gemma-3-1b-it  \n",
        "**Technique**: QLoRA with Unsloth (Dynamic 4-bit)  \n",
        "**Hardware**: Google Colab T4 GPU (Free Tier)  \n",
        "\n",
        "---\n",
        "\n",
        "## üìö What You'll Learn\n",
        "\n",
        "1. **Why Gemma 3 1B?** - Modern SLM optimized for efficiency\n",
        "2. **What is Unsloth?** - How it makes training 2x faster with 70% less VRAM\n",
        "3. **QLoRA Explained** - Low-rank adaptation for efficient fine-tuning\n",
        "4. **4-bit Quantization** - How to compress models without losing accuracy\n",
        "5. **Dynamic Quantization** - Unsloth's smart approach to preserving critical weights\n",
        "6. **GGUF Format** - Converting for CPU inference with llama.cpp\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Training Objective\n",
        "\n",
        "Fine-tune Gemma 3 1B to translate natural language ‚Üí venvy CLI commands:\n",
        "\n",
        "```\n",
        "Input:  \"list all environments sorted by size\"\n",
        "Output: \"venvy ls --sort size\"\n",
        "```\n",
        "\n",
        "**Target Accuracy**: 80-90% on domain-specific commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkz1nKE7OEu0"
      },
      "source": [
        "---\n",
        "\n",
        "# Step 1: Setup and Installation\n",
        "\n",
        "## üîß Install Unsloth and Dependencies\n",
        "\n",
        "### What is Unsloth?\n",
        "\n",
        "**Unsloth** is a highly optimized library for fine-tuning LLMs that provides:\n",
        "\n",
        "- **2x Faster Training**: Custom CUDA kernels optimized for LoRA operations\n",
        "- **70% Less VRAM**: Efficient memory management and gradient checkpointing\n",
        "- **Dynamic 4-bit Quantization**: Smart weight selection (don't quantize critical layers)\n",
        "- **Zero Accuracy Loss**: Maintains full precision where it matters\n",
        "\n",
        "### How Unsloth Works:\n",
        "\n",
        "```\n",
        "Traditional Fine-tuning:\n",
        "‚îú‚îÄ‚îÄ Load full model (FP16) ‚Üí 2.2GB VRAM\n",
        "‚îú‚îÄ‚îÄ Compute gradients for ALL parameters\n",
        "‚îî‚îÄ‚îÄ Update all 1.1B parameters ‚Üí SLOW\n",
        "\n",
        "Unsloth + QLoRA:\n",
        "‚îú‚îÄ‚îÄ Load model in 4-bit ‚Üí 650MB VRAM\n",
        "‚îú‚îÄ‚îÄ Add small LoRA adapters (8-16MB)\n",
        "‚îú‚îÄ‚îÄ Compute gradients ONLY for adapters ‚Üí FAST\n",
        "‚îî‚îÄ‚îÄ Update <1% of parameters ‚Üí 2x speed, 70% less VRAM\n",
        "```\n",
        "\n",
        "### Dynamic 4-bit Quantization:\n",
        "\n",
        "Unsloth analyzes your model and **selectively avoids quantizing** critical layers:\n",
        "- Attention output layers\n",
        "- Layer norms\n",
        "- Embedding layers\n",
        "\n",
        "Result: **10% more VRAM but significantly better accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-pm08x2aOEu2"
      },
      "outputs": [],
      "source": [
        "# Install Unsloth with all optimizations\n",
        "# This will take ~3-5 minutes on first run\n",
        "\n",
        "%%capture\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "\n",
        "print(\"‚úÖ Unsloth and dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moNI7_5kOEu3",
        "outputId": "c2949121-6e77-40de-cd3e-4775b10ac9d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GPU Available: Tesla T4\n",
            "   Total VRAM: 15.8 GB\n"
          ]
        }
      ],
      "source": [
        "# Verify GPU is available\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"‚úÖ GPU Available: {gpu_name}\")\n",
        "    print(f\"   Total VRAM: {gpu_memory:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ùå No GPU detected! This notebook requires a GPU.\")\n",
        "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí Select T4 GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ-cqNghOEu3"
      },
      "source": [
        "---\n",
        "\n",
        "# Step 2: Clone Repository and Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5LUKDrgOEu4",
        "outputId": "b0d8a566-c4d2-43bf-a605-2baaf79a5676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nlcli-wizard'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 134 (delta 6), reused 23 (delta 4), pack-reused 101 (from 1)\u001b[K\n",
            "Receiving objects: 100% (134/134), 77.46 MiB | 15.53 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Updating files: 100% (117/117), done.\n",
            "Encountered 1 file(s) that should have been pointers, but weren't:\n",
            "\tmodels/readme.md\n",
            "\n",
            "‚úÖ Repository cloned successfully!\n",
            "   Current directory: /content/nlcli-wizard\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pranavkumaarofficial/nlcli-wizard.git\n",
        "import os\n",
        "# Change to project directory\n",
        "os.chdir('/content/nlcli-wizard')\n",
        "\n",
        "print(\"\\n‚úÖ Repository cloned successfully!\")\n",
        "print(f\"   Current directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btNhWP0zOEu4",
        "outputId": "9cb9e2e6-7195-4ede-ef95-485d3c7aba8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset loaded: 1500 examples\n",
            "\n",
            "üìã Sample Examples:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 1:\n",
            "  Instruction: Translate to venvy command: show current environment\n",
            "  Output: COMMAND: venvy current\n",
            "CONFIDENCE: 0.97\n",
            "EXPLANATION: Shows currently active virtual environment\n",
            "\n",
            "Example 2:\n",
            "  Instruction: Translate to venvy command: sort environments by size\n",
            "  Output: COMMAND: venvy ls -s size\n",
            "CONFIDENCE: 0.91\n",
            "EXPLANATION: Lists environments sorted by disk space used\n",
            "\n",
            "Example 3:\n",
            "  Instruction: Translate to venvy command: which venv am i using\n",
            "  Output: COMMAND: venvy current\n",
            "CONFIDENCE: 0.95\n",
            "EXPLANATION: Shows currently active virtual environment\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Verify dataset exists and inspect it\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "dataset_path = Path(\"data/venvy_training.jsonl\")\n",
        "\n",
        "if not dataset_path.exists():\n",
        "    print(\"‚ùå Dataset not found! Make sure you pushed data/venvy_training.jsonl to GitHub\")\n",
        "else:\n",
        "    # Load and inspect dataset\n",
        "    examples = []\n",
        "    with open(dataset_path, 'r') as f:\n",
        "        for line in f:\n",
        "            examples.append(json.loads(line))\n",
        "\n",
        "    print(f\"‚úÖ Dataset loaded: {len(examples)} examples\")\n",
        "    print(\"\\nüìã Sample Examples:\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for i, ex in enumerate(examples[:3]):\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"  Instruction: {ex['instruction']}\")\n",
        "        print(f\"  Output: {ex['output'].strip()}\")\n",
        "\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sYAtWCrOEu5"
      },
      "source": [
        "---\n",
        "\n",
        "# Step 3: Load Gemma 3 1B with Unsloth\n",
        "\n",
        "## üìñ Understanding Model Loading\n",
        "\n",
        "### What happens when we load a model?\n",
        "\n",
        "1. **Download from HuggingFace** (~2.2GB for Gemma 3 1B in FP16)\n",
        "2. **Load into GPU memory** with quantization\n",
        "3. **Prepare for training** with LoRA adapters\n",
        "\n",
        "### Quantization Explained:\n",
        "\n",
        "**Normal Precision (FP16)**:\n",
        "```\n",
        "Weight: 0.123456789 (16 bits) ‚Üí 2 bytes per parameter\n",
        "1.1B parameters √ó 2 bytes = 2.2 GB\n",
        "```\n",
        "\n",
        "**4-bit Quantization (NF4)**:\n",
        "```\n",
        "Weight: 0.123456789 ‚Üí Quantized to 4 bits (0-15)\n",
        "1.1B parameters √ó 0.5 bytes = 550 MB\n",
        "```\n",
        "\n",
        "**NF4 (Normal Float 4-bit)**:\n",
        "- Special quantization format optimized for neural network weights\n",
        "- Weights follow normal distribution, so use non-uniform quantization\n",
        "- More precision for common values, less for outliers\n",
        "\n",
        "### Dynamic 4-bit:\n",
        "\n",
        "Unsloth's smart feature:\n",
        "```python\n",
        "if layer_is_critical():  # Attention, embeddings, norms\n",
        "    keep_fp16()  # Don't quantize\n",
        "else:\n",
        "    quantize_4bit()  # Safe to compress\n",
        "```\n",
        "\n",
        "Result: **~650MB VRAM** (instead of 2.2GB) with minimal accuracy loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629,
          "referenced_widgets": [
            "58c0b00caae74705b4cec63a68057176",
            "f6aa55c1416240c19abc8a036f530eb4",
            "31f725f224114ac4a6a386ac15cdd455",
            "f982825f9251462b98bb8b7549eb745a",
            "087d014f07ca478888687a341625c893",
            "09de1be93d3647faafc777fc7a8c82a3",
            "9e347f85d56d43b3a31863e832f967c1",
            "f6354ea3438c4f85947a3f098ff269b8",
            "ee082d176a5f4d099842c3d0c033828c",
            "02dea77f533f4e1fb56876f61f4d4009",
            "d27997e3675d40efb024c056cda6d79e",
            "0329d70c15134b19979a07d0ec26a65a",
            "7bcdfd3449544c928baad9d00b5a5a08",
            "d28a133d0acf43fba1d2ebe833751865",
            "b96eb8224548459c88019a771ed0b7ad",
            "ea4f1a76bbf148818848cd9fb08443f7",
            "f724e469582d4195923d33b2847e3632",
            "951b7ed0401e490295f05e40e16ce500",
            "3fe16f70bc2d4101ab7fc5be9d7c1528",
            "fdb0cd8b8e024997a8d00058b30ac5bb",
            "bafc3c9850084c91ba20af59f588acc9",
            "acc001a0d08c4fcfb8bc2adfb0a64e6b",
            "93821e90be744ce28ddfb8de178d3a12",
            "c0eaa29f67e9460ba2bb2caa931d7013",
            "3cc019342cca4e1da664977411a45d81",
            "ed17f1a1dfbd420fbd8d5a4ddbe29490",
            "825d9b69c30240819c51cd0a5b996d79",
            "32c67b09a77c4476853063ca928b546f",
            "e9c6e9d3470843a48b7a9200aae617b1",
            "4e55d34c2bac4d9f8ea28129cc5cdb69",
            "d4e24dfe333f46fcab5b451c5ca2c566",
            "9787a57a776b46ddb0a99694e8385a6a",
            "6b9d5ba50fd54d95a5ee1ace2ad460ba",
            "f68ebd74ff2b462f89814deb09b096f1",
            "e6160865ba044dc98d6d37de5a81622f",
            "d81cd324c368470b9d8f729fcd1ae711",
            "e7002e44893f4ab390e38101bd2aab4a",
            "53cf547e1f7c4d0fb56756871261e875",
            "51fceabbb510478ab1cd2b0eb6bc107b",
            "1efef71ffb344721a138ed9b78ac426f",
            "233c60cfca7c441eabcc440ea290b3db",
            "52c087f93f254fc5abe94958b4018762",
            "2decb2e6d39043179cee49f6358bef7b",
            "64b3be1d0fc146848243c69ae1c8bc48",
            "4516c29e8acf4f08a6a1fade33ab78aa",
            "ddd2681a2bac486397110a3f0284cc41",
            "471f236f23834677acd3a4fa65b704f7",
            "328793a850234612a97450618e380db9",
            "44035936bb50474d958a417f32405a74",
            "a82344ace7b84f4fb2700f8e1f568caa",
            "894a0ca53c794a528773378098b25e05",
            "4245fc8a5a9543d88f62cd489ddd36f9",
            "1e2eefab6ec14718aefc79b999e19afd",
            "6b9deff0cec74a129d7fd5dbb3be4016",
            "ea948544ce3b4d1b8bc5d7c6729fd691",
            "d871e02c835b41a6818201d29ac39519",
            "848b3a570f15463bb1a5d8aaf63a94e1",
            "505fb0c10c8149729b851208b74d5fb9",
            "317bcd6724ff460aa673fd9ed1309069",
            "d495504d94dc4c7abb680afa1dedca20",
            "ab4f1ae9a8d9465baf0d1b972a061242",
            "a58fd094c3cf4b9aa6adbb6af2741daf",
            "32351141b5cf4b00aeb348651db061a4",
            "ad31a8ed6940464489246dac083dc1fd",
            "9c5fd844742b4ae18467e58a78acddb9",
            "28c8e954be7148c6abe3a80b1698e693",
            "612260e6f8644210ae4e0795c8b37809",
            "3ec7d8cafa4445bebea57b7bd75ad916",
            "e8ea2a14d69b43d2aada34e0385a8e36",
            "8e5c9726f79546d282419dd4e90c255e",
            "9dc1696e897d4e1e8fe5b41824ba6286",
            "87bca939e938497fa7f2bd8eac3817ee",
            "1d5209a61b4045cea059103dbeb5a93a",
            "3cc8e9daa6394d6a905ce7d4d56236cd",
            "ecf6380592c64d6399e509871c0f398b",
            "26e1669317cc46d1b2f3a698b8be9652",
            "82fe3cb16b934683a302061b28d49732"
          ]
        },
        "id": "_caeE-nqOEu5",
        "outputId": "8e7e0201-b393-4f66-f549-2552b0207d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "üîÑ Loading Gemma 3 1B with Unsloth optimizations...\n",
            "   Model: unsloth/gemma-3-1b-it\n",
            "   Max sequence length: 512\n",
            "   4-bit quantization: True\n",
            "\n",
            "‚è≥ This will take 2-3 minutes (downloading ~2.2GB)...\n",
            "\n",
            "==((====))==  Unsloth 2025.11.2: Fast Gemma3 patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n",
            "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58c0b00caae74705b4cec63a68057176"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0329d70c15134b19979a07d0ec26a65a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93821e90be744ce28ddfb8de178d3a12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f68ebd74ff2b462f89814deb09b096f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4516c29e8acf4f08a6a1fade33ab78aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d871e02c835b41a6818201d29ac39519"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "612260e6f8644210ae4e0795c8b37809"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Model loaded successfully!\n",
            "   Model parameters: 999,885,952\n",
            "   Memory allocated: 1.00 GB\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Model configuration\n",
        "model_name = \"unsloth/gemma-3-1b-it\"  # Unsloth's optimized version\n",
        "max_seq_length = 512  # Maximum context length for our task\n",
        "dtype = None  # Auto-detect (FP16 for T4 GPU)\n",
        "load_in_4bit = True  # Enable 4-bit quantization\n",
        "\n",
        "print(\"üîÑ Loading Gemma 3 1B with Unsloth optimizations...\")\n",
        "print(f\"   Model: {model_name}\")\n",
        "print(f\"   Max sequence length: {max_seq_length}\")\n",
        "print(f\"   4-bit quantization: {load_in_4bit}\")\n",
        "print(\"\\n‚è≥ This will take 2-3 minutes (downloading ~2.2GB)...\\n\")\n",
        "\n",
        "# Load model with Unsloth\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    # Dynamic 4-bit: Don't quantize critical layers\n",
        "    # This uses ~10% more VRAM but improves accuracy by 15-20%\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Model loaded successfully!\")\n",
        "print(f\"   Model parameters: {model.num_parameters():,}\")\n",
        "print(f\"   Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So0N_l45OEu6"
      },
      "source": [
        "---\n",
        "\n",
        "# Step 4: Add LoRA Adapters\n",
        "\n",
        "## üìñ Understanding LoRA (Low-Rank Adaptation)\n",
        "\n",
        "### The Problem:\n",
        "Traditional fine-tuning updates **ALL 1.1 billion parameters**:\n",
        "- Requires massive memory (store gradients for 1.1B params)\n",
        "- Very slow (update 1.1B weights)\n",
        "- Easy to overfit on small datasets\n",
        "\n",
        "### LoRA Solution:\n",
        "Instead of modifying original weights, add **small adapter matrices**:\n",
        "\n",
        "```\n",
        "Original Weight Matrix W (large):\n",
        "[1024 √ó 1024] = 1,048,576 parameters\n",
        "\n",
        "LoRA Decomposition:\n",
        "ŒîW = A √ó B\n",
        "A: [1024 √ó 8]  = 8,192 parameters\n",
        "B: [8 √ó 1024]  = 8,192 parameters\n",
        "Total: 16,384 parameters (64x smaller!)\n",
        "\n",
        "Final Output:\n",
        "y = W¬∑x + Œ±¬∑(A¬∑B)¬∑x\n",
        "    ‚Üë      ‚Üë\n",
        " frozen  trainable\n",
        "```\n",
        "\n",
        "### Key Parameters:\n",
        "\n",
        "1. **r (rank)**: Size of adapter matrices (typically 8-16)\n",
        "   - Higher r = more capacity but slower\n",
        "   - Lower r = faster but less expressive\n",
        "   - We use r=16 (good balance)\n",
        "\n",
        "2. **lora_alpha**: Scaling factor for LoRA updates\n",
        "   - Controls how much LoRA affects output\n",
        "   - Typically 2√ór (we use 32)\n",
        "\n",
        "3. **lora_dropout**: Regularization (prevent overfitting)\n",
        "   - We use 0 (dataset is diverse enough)\n",
        "\n",
        "4. **target_modules**: Which layers to adapt\n",
        "   - `q_proj`, `k_proj`: Query/Key attention projections\n",
        "   - `v_proj`, `o_proj`: Value/Output projections\n",
        "   - `gate_proj`, `up_proj`, `down_proj`: MLP layers\n",
        "\n",
        "### Memory Savings:\n",
        "```\n",
        "Without LoRA: 1.1B params √ó 2 bytes = 2.2 GB\n",
        "With LoRA: 8M params √ó 2 bytes = 16 MB\n",
        "\n",
        "Savings: 99.3% reduction in trainable parameters!\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbtfhRfyOEu6",
        "outputId": "159bf117-d6c6-4daf-c41f-4e152c33fd80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `model.base_model.model.model` require gradients\n",
            "‚úÖ LoRA adapters added!\n",
            "\n",
            "üìä Model Statistics:\n",
            "   Total parameters: 675,994,752\n",
            "   Trainable parameters: 13,045,760\n",
            "   Trainable %: 1.93%\n",
            "   Memory allocated: 1.05 GB\n",
            "\n",
            "üí° Insight:\n",
            "   We're training only 1.93% of parameters!\n",
            "   This is why LoRA is so efficient.\n"
          ]
        }
      ],
      "source": [
        "# Add LoRA adapters to the model\n",
        "# These are small matrices we'll train instead of the full model\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank (size of adapter matrices)\n",
        "    target_modules=[\n",
        "        \"q_proj\",     # Query projection in attention\n",
        "        \"k_proj\",     # Key projection\n",
        "        \"v_proj\",     # Value projection\n",
        "        \"o_proj\",     # Output projection\n",
        "        \"gate_proj\",  # MLP gate\n",
        "        \"up_proj\",    # MLP up\n",
        "        \"down_proj\",  # MLP down\n",
        "    ],\n",
        "    lora_alpha=32,  # LoRA scaling factor (typically 2√ór)\n",
        "    lora_dropout=0,  # No dropout (our dataset is diverse)\n",
        "    bias=\"none\",     # Don't train bias terms\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Memory optimization\n",
        "    random_state=42,  # Reproducibility\n",
        "    use_rslora=False,  # Standard LoRA (RSLoRA is for very large models)\n",
        "    loftq_config=None,  # No LoftQ quantization\n",
        ")\n",
        "\n",
        "print(\"‚úÖ LoRA adapters added!\")\n",
        "print(\"\\nüìä Model Statistics:\")\n",
        "\n",
        "# Count trainable vs frozen parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_pct = 100 * trainable_params / total_params\n",
        "\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"   Trainable %: {trainable_pct:.2f}%\")\n",
        "print(f\"   Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "\n",
        "print(\"\\nüí° Insight:\")\n",
        "print(f\"   We're training only {trainable_pct:.2f}% of parameters!\")\n",
        "print(f\"   This is why LoRA is so efficient.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltL-qEO6OEu6"
      },
      "source": [
        "---\n",
        "\n",
        "# Step 5: Prepare Dataset for Training\n",
        "\n",
        "## üìñ Understanding the Training Format\n",
        "\n",
        "### Alpaca Format:\n",
        "Our dataset uses the Alpaca instruction format:\n",
        "```json\n",
        "{\n",
        "  \"instruction\": \"Task description\",\n",
        "  \"input\": \"Additional context (empty for us)\",\n",
        "  \"output\": \"Expected response\"\n",
        "}\n",
        "```\n",
        "\n",
        "### How it's converted for training:\n",
        "```\n",
        "Alpaca Format:\n",
        "  instruction: \"Translate to venvy command: list all environments\"\n",
        "  input: \"\"\n",
        "  output: \"COMMAND: venvy ls\\nCONFIDENCE: 0.95\\n...\"\n",
        "\n",
        "‚Üì Transformed to ‚Üì\n",
        "\n",
        "Gemma 3 Chat Format:\n",
        "<start_of_turn>user\n",
        "Translate to venvy command: list all environments<end_of_turn>\n",
        "<start_of_turn>model\n",
        "COMMAND: venvy ls\n",
        "CONFIDENCE: 0.95\n",
        "EXPLANATION: Lists all registered virtual environments\n",
        "<end_of_turn>\n",
        "```\n",
        "\n",
        "### Why this format?\n",
        "- Gemma 3 is trained as a chat model with turn-based conversation\n",
        "- `<start_of_turn>user` signals user input\n",
        "- `<start_of_turn>model` signals model response\n",
        "- This matches how Gemma 3 was pre-trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254,
          "referenced_widgets": [
            "280ca4a361124b029058eb9266360eae",
            "1b91f1c17e2b4e518e1ef00dd90f76eb",
            "75d899987b844a3fa06c23d464f92a41",
            "5bb0fa44a8ce436e9f318dedca2aca42",
            "36c5153b9caf4bafa5783786fc5510ad",
            "48f36b53e1f04014b03b0b1286a56817",
            "243c2aefb0f8410a8cb576ad0494525a",
            "a1a5fc3155a34dadabe48106233772db",
            "541599a1cd1241a7bcb3332ed5966d00",
            "dbd44b2eee934c60b4390cee1534292e",
            "27753014a8ff42e1baa263dd96220f66"
          ]
        },
        "id": "8vhNMLAzOEu6",
        "outputId": "6c6006fa-9ec9-4b0b-b553-37d20629a4e1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "280ca4a361124b029058eb9266360eae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset loaded: 1500 examples\n",
            "\n",
            "üìã Dataset Structure:\n",
            "Dataset({\n",
            "    features: ['instruction', 'input', 'output'],\n",
            "    num_rows: 1500\n",
            "})\n",
            "\n",
            "üìù Sample Example:\n",
            "{'instruction': 'Translate to venvy command: show current environment', 'input': '', 'output': 'COMMAND: venvy current\\nCONFIDENCE: 0.97\\nEXPLANATION: Shows currently active virtual environment\\n'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset from JSONL file\n",
        "dataset = load_dataset('json', data_files='data/venvy_training.jsonl', split='train')\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded: {len(dataset)} examples\")\n",
        "print(\"\\nüìã Dataset Structure:\")\n",
        "print(dataset)\n",
        "print(\"\\nüìù Sample Example:\")\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX4zCZZ9OEu6",
        "outputId": "2624713b-7af9-4411-d6f9-22fa274b0b59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset split:\n",
            "   Training examples: 1350\n",
            "   Validation examples: 150\n",
            "\n",
            "üí° Why validation set?\n",
            "   We'll evaluate on this during training to detect overfitting.\n",
            "   If validation loss stops improving, we stop training.\n"
          ]
        }
      ],
      "source": [
        "# Split dataset: 90% train, 10% validation\n",
        "# Validation set helps us monitor if the model is overfitting\n",
        "\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = dataset['train']\n",
        "eval_dataset = dataset['test']\n",
        "\n",
        "print(f\"‚úÖ Dataset split:\")\n",
        "print(f\"   Training examples: {len(train_dataset)}\")\n",
        "print(f\"   Validation examples: {len(eval_dataset)}\")\n",
        "\n",
        "print(\"\\nüí° Why validation set?\")\n",
        "print(\"   We'll evaluate on this during training to detect overfitting.\")\n",
        "print(\"   If validation loss stops improving, we stop training.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302,
          "referenced_widgets": [
            "711faff2256e4788893512683ab1203f",
            "ab06957e76d14f0abece120932878cde",
            "d1743f715a8b426aa13370d360943a83",
            "0442381ee1014781b7025af4dff4b5ae",
            "805e4a5d1acd4447aaaa793dd076cde6",
            "47ac8a4565e542ba8f57aeb631078037",
            "1e69dfdc92a647f1865d2c8f9683e892",
            "97d7f2b08cb140c2800fc1d7f66ec4d1",
            "ceb3907b37134d9c98f4ecb208d1cb14",
            "a274683b5ecf42a2b453a46c2a934a28",
            "f2062dcfe1314122ba847fe2b4e762c6",
            "7ef6f2edaeb34d6784d17b8df1006372",
            "2c99a956b7ed41d8aaf3f399bafa111b",
            "a0eb193e1f9f4ada9205e3391722dbbf",
            "c9e73ea2f1504ba2b2d7169abbb6ec62",
            "c480611d7d0f42c685a7bfe60c1c61b7",
            "12a2f2b57ae94e3dba6e558184f049ed",
            "ecb18210a2ba4972900e58a410c9646b",
            "8302d51430664f3ea095dd2ee30c8a90",
            "c4e06b319ea84734abc4690e61fa0b4e",
            "d9e74ba984e44ab09f680c5cf5e1d33c",
            "65939adc0f9645aca7400bcfcae5eabb"
          ]
        },
        "id": "7HMzVXdwOEu7",
        "outputId": "6effab0a-5028-416f-eb13-5b0464571b51"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1350 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "711faff2256e4788893512683ab1203f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ef6f2edaeb34d6784d17b8df1006372"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset formatted for Gemma 3 chat!\n",
            "\n",
            "üìù Formatted Example:\n",
            "--------------------------------------------------------------------------------\n",
            "<start_of_turn>user\n",
            "Translate to venvy command: preview cleanup<end_of_turn>\n",
            "<start_of_turn>model\n",
            "COMMAND: venvy cleanup --dry-run\n",
            "CONFIDENCE: 0.97\n",
            "EXPLANATION: Shows which environments would be removed without deleting\n",
            "<end_of_turn><end_of_turn>\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Format dataset for Gemma 3 chat format\n",
        "# This converts our Alpaca format to Gemma's expected input format\n",
        "\n",
        "# Gemma 3 chat template\n",
        "alpaca_prompt = \"\"\"<start_of_turn>user\n",
        "{}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "{}<end_of_turn>\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token  # End-of-sequence token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"\n",
        "    Convert Alpaca format to Gemma 3 chat format.\n",
        "\n",
        "    For each example:\n",
        "    1. Combine instruction + input (input is empty for us)\n",
        "    2. Format as Gemma chat turn\n",
        "    3. Add EOS token for proper training\n",
        "    \"\"\"\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "\n",
        "    texts = []\n",
        "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
        "        # Combine instruction and input (input is empty for our dataset)\n",
        "        full_instruction = instruction + (\"\\n\" + input_text if input_text else \"\")\n",
        "\n",
        "        # Format as chat turns\n",
        "        text = alpaca_prompt.format(full_instruction, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Apply formatting to both train and validation sets\n",
        "train_dataset = train_dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        ")\n",
        "\n",
        "eval_dataset = eval_dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Dataset formatted for Gemma 3 chat!\")\n",
        "print(\"\\nüìù Formatted Example:\")\n",
        "print(\"-\" * 80)\n",
        "print(train_dataset[0]['text'])\n",
        "print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB9CKxVPOEu7"
      },
      "source": [
        "---\n",
        "\n",
        "# Step 6: Configure Training Parameters\n",
        "\n",
        "## üìñ Understanding Hyperparameters\n",
        "\n",
        "### Key Training Parameters:\n",
        "\n",
        "#### 1. **Learning Rate (lr)**: How fast the model learns\n",
        "```\n",
        "Too high (1e-3):  Model diverges, loss explodes\n",
        "Just right (2e-4): Smooth learning, converges well\n",
        "Too low (1e-5):   Learns too slowly, wastes time\n",
        "```\n",
        "We use **2e-4** (0.0002) - standard for LoRA fine-tuning.\n",
        "\n",
        "#### 2. **Batch Size**: How many examples per update\n",
        "```\n",
        "per_device_batch_size=4:  Process 4 examples at once\n",
        "gradient_accumulation_steps=4: Accumulate 4 batches\n",
        "Effective batch size = 4 √ó 4 = 16\n",
        "```\n",
        "Why split?\n",
        "- T4 GPU has 16GB VRAM\n",
        "- Batch size 16 would cause OOM (out of memory)\n",
        "- So we process 4 at a time, accumulate gradients, then update\n",
        "\n",
        "#### 3. **Epochs**: How many times to see full dataset\n",
        "```\n",
        "1 epoch = model sees each example once\n",
        "3 epochs = model sees each example 3 times\n",
        "```\n",
        "We use **3 epochs** - enough to learn without overfitting.\n",
        "\n",
        "#### 4. **Weight Decay**: Regularization to prevent overfitting\n",
        "```\n",
        "weight_decay=0.01: Small penalty on large weights\n",
        "```\n",
        "Encourages model to use many small weights instead of few large ones.\n",
        "\n",
        "#### 5. **Learning Rate Schedule**: Warmup + Cosine Decay\n",
        "```\n",
        "Step 0-50:    Warmup (gradual increase) ‚Üí Prevents early instability\n",
        "Step 50-end:  Cosine decay (gradual decrease) ‚Üí Better convergence\n",
        "\n",
        "Learning Rate over time:\n",
        "    |\n",
        "2e-4|        _______________\n",
        "    |      /                 \\\n",
        "    |    /                     \\\n",
        "    |  /                         \\\n",
        "  0 |_/____________________________\\_____\n",
        "     0   50                   1000  steps\n",
        "```\n",
        "\n",
        "#### 6. **Mixed Precision (FP16)**: Speed + Memory optimization\n",
        "```\n",
        "Normal (FP32):  32 bits per number ‚Üí Slow but accurate\n",
        "Mixed (FP16):   16 bits per number ‚Üí 2x faster, 2x less memory\n",
        "```\n",
        "T4 GPU has FP16 cores (Tensor Cores) ‚Üí much faster.\n",
        "\n",
        "### Expected Training Time:\n",
        "```\n",
        "1,350 examples √ó 3 epochs = 4,050 training steps\n",
        "4,050 / (batch_size 16) = ~253 update steps\n",
        "~1-2 seconds per step on T4\n",
        "Total: ~8-10 minutes\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMc0mgnXOEu7",
        "outputId": "b9cb6126-a5b3-4749-ebf5-7d64941d877d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Training configuration set!\n",
            "\n",
            "üìä Training Summary:\n",
            "   Epochs: 3\n",
            "   Effective batch size: 16\n",
            "   Learning rate: 0.0002\n",
            "   Warmup steps: 50\n",
            "   FP16 enabled: True\n",
            "\n",
            "‚è±Ô∏è Estimated training time:\n",
            "   Total steps: ~253\n",
            "   Time: ~8.4 minutes (assuming 2 sec/step)\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Training configuration\n",
        "training_args = TrainingArguments(\n",
        "    # Output and logging\n",
        "    output_dir=\"./outputs\",              # Where to save model checkpoints\n",
        "    logging_dir=\"./logs\",                # Where to save logs\n",
        "    logging_steps=10,                    # Log every 10 steps\n",
        "\n",
        "    # Training hyperparameters\n",
        "    num_train_epochs=3,                  # Train for 3 epochs\n",
        "    per_device_train_batch_size=4,       # 4 examples per GPU\n",
        "    gradient_accumulation_steps=4,       # Accumulate 4 batches (effective batch=16)\n",
        "    learning_rate=2e-4,                  # Standard LoRA learning rate\n",
        "    weight_decay=0.01,                   # L2 regularization\n",
        "\n",
        "    # Learning rate schedule\n",
        "    lr_scheduler_type=\"cosine\",          # Cosine decay schedule\n",
        "    warmup_steps=50,                     # Warmup for first 50 steps\n",
        "\n",
        "    # Optimization\n",
        "    optim=\"adamw_8bit\",                  # 8-bit AdamW (saves memory)\n",
        "    fp16=True,                           # Mixed precision training (2x faster)\n",
        "\n",
        "    # Evaluation\n",
        "    eval_strategy=\"steps\",               # Evaluate during training\n",
        "    eval_steps=50,                       # Evaluate every 50 steps\n",
        "    per_device_eval_batch_size=4,        # Batch size for evaluation\n",
        "\n",
        "    # Checkpointing\n",
        "    save_strategy=\"steps\",               # Save checkpoints\n",
        "    save_steps=100,                      # Save every 100 steps\n",
        "    save_total_limit=3,                  # Keep only 3 best checkpoints\n",
        "    load_best_model_at_end=True,         # Load best checkpoint at end\n",
        "    metric_for_best_model=\"eval_loss\",   # Use validation loss to pick best\n",
        "\n",
        "    # Memory optimizations\n",
        "    gradient_checkpointing=True,         # Save memory (slight speed cost)\n",
        "    max_grad_norm=1.0,                   # Gradient clipping (stability)\n",
        "\n",
        "    # Reproducibility\n",
        "    seed=42,\n",
        "\n",
        "    # Disable unnecessary features\n",
        "    report_to=\"none\",                    # Don't report to wandb/tensorboard\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training configuration set!\")\n",
        "print(\"\\nüìä Training Summary:\")\n",
        "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"   Warmup steps: {training_args.warmup_steps}\")\n",
        "print(f\"   FP16 enabled: {training_args.fp16}\")\n",
        "\n",
        "# Calculate approximate training time\n",
        "total_steps = (len(train_dataset) * training_args.num_train_epochs) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\n",
        "print(f\"\\n‚è±Ô∏è Estimated training time:\")\n",
        "print(f\"   Total steps: ~{total_steps}\")\n",
        "print(f\"   Time: ~{total_steps * 2 / 60:.1f} minutes (assuming 2 sec/step)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F6gqACeOEu7"
      },
      "source": [
        "---\n",
        "\n",
        "# Step 7: Train the Model! üöÄ\n",
        "\n",
        "## üìñ What Happens During Training?\n",
        "\n",
        "### Training Loop:\n",
        "```python\n",
        "for epoch in range(3):\n",
        "    for batch in train_dataset:\n",
        "        # 1. Forward pass: Get model predictions\n",
        "        predictions = model(batch)\n",
        "        \n",
        "        # 2. Calculate loss: How wrong are we?\n",
        "        loss = cross_entropy(predictions, targets)\n",
        "        \n",
        "        # 3. Backward pass: Calculate gradients\n",
        "        gradients = loss.backward()\n",
        "        \n",
        "        # 4. Update LoRA weights\n",
        "        optimizer.step(gradients)\n",
        "        \n",
        "        # 5. Log progress\n",
        "        if step % 10 == 0:\n",
        "            print(f\"Loss: {loss:.4f}\")\n",
        "```\n",
        "\n",
        "### What to Watch:\n",
        "\n",
        "1. **Training Loss**: Should decrease smoothly\n",
        "   ```\n",
        "   Good:    2.5 ‚Üí 1.8 ‚Üí 1.2 ‚Üí 0.8 ‚Üí 0.5\n",
        "   Bad:     2.5 ‚Üí 5.8 ‚Üí NaN (model diverged!)\n",
        "   ```\n",
        "\n",
        "2. **Validation Loss**: Should also decrease\n",
        "   ```\n",
        "   Good:    Train loss ‚âà Val loss (not overfitting)\n",
        "   Bad:     Train 0.3, Val 2.5 (overfitting!)\n",
        "   ```\n",
        "\n",
        "3. **Speed**: Should be ~1-2 seconds per step\n",
        "   - Slower? GPU not being used efficiently\n",
        "   - Faster? Might be skipping computation\n",
        "\n",
        "### Training Metrics Explained:\n",
        "\n",
        "- **loss**: Cross-entropy loss (lower = better)\n",
        "- **learning_rate**: Current LR (starts low, increases, then decreases)\n",
        "- **epoch**: Which epoch we're on (0-3)\n",
        "- **grad_norm**: Gradient magnitude (should be stable, not exploding)\n",
        "\n",
        "This cell will take ~8-10 minutes. Grab a coffee! ‚òï"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229,
          "referenced_widgets": [
            "7bf7332fda8f42ffa2dfbdf8a90cc398",
            "58fad93163194d91acfb3540840b8502",
            "c4dc028e95a24d3a87d44319cf6e0da0",
            "a8cbe9bc4bab4f9e8a5d1f6096399b27",
            "3732151639fa4d739b12121e48b3d14b",
            "f8610d57adf646cdbf954eec3a0ae7ed",
            "023e99de1a75494bb7491631dcf296fc",
            "0d48cd0fd7d64f29ac45f51f785a8078",
            "27097ab156c54b6bb0003be7e4f11d22",
            "24eb81674cb04959b499814b09178964",
            "5ec221d57968463e9fbb7cec0c43d16e",
            "ea42129706cb4083acc5a77d5e705599",
            "4e3b486b413544f8a2a5f074e3bd9acc",
            "7c3e7736a9174855807a7e17c501a2c8",
            "6b07d7ceeb1440a290829f7b9ae4ff92",
            "4c42672cedf94310ad65a93f5daf5cd1",
            "93f57d2217fb4b0bb1b67e4f31b9be87",
            "20cd791d40814a49bd6e72efad1baee5",
            "38e39bbb997d4069b4b240963d971eb1",
            "25470b84989f44ecb2939c024fa9cae4",
            "a9eff16482fc47c9997f878168423f75",
            "e45d94832b1f4b82bb25a7ffe8d9e9df"
          ]
        },
        "id": "Uvmq2g-VOEu8",
        "outputId": "e4917864-aa6d-493e-cd5b-076fd59625f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Switching to float32 training since model cannot work with float16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/1350 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bf7332fda8f42ffa2dfbdf8a90cc398"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/150 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea42129706cb4083acc5a77d5e705599"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Trainer initialized!\n",
            "\n",
            "üöÄ Starting training...\n",
            "   This will take ~8-10 minutes on T4 GPU\n",
            "   Watch the loss decrease over time!\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Create trainer with SFTTrainer (Supervised Fine-Tuning)\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    dataset_text_field=\"text\",  # Which field contains the formatted text\n",
        "    max_seq_length=max_seq_length,\n",
        "    args=training_args,\n",
        "    packing=False,  # Don't pack multiple examples (our examples are short)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer initialized!\")\n",
        "print(\"\\nüöÄ Starting training...\")\n",
        "print(\"   This will take ~8-10 minutes on T4 GPU\")\n",
        "print(\"   Watch the loss decrease over time!\")\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "kYJXqx-OOEu8",
        "outputId": "6a53ff4e-d409-4619-a623-d75275d2fd8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,350 | Num Epochs = 3 | Total steps = 255\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 13,045,760 of 1,012,931,712 (1.29% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='255' max='255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [255/255 09:18, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.497600</td>\n",
              "      <td>0.368994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.193300</td>\n",
              "      <td>0.181701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.155700</td>\n",
              "      <td>0.148017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.139300</td>\n",
              "      <td>0.142402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.135100</td>\n",
              "      <td>0.138184</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Not an error, but Gemma3ForCausalLM does not accept `num_items_in_batch`.\n",
            "Using gradient accumulation will be very slightly less accurate.\n",
            "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "üéâ Training complete!\n",
            "\n",
            "üìä Final Statistics:\n",
            "   Train runtime: 617.72 seconds\n",
            "   Train samples/second: 6.56\n",
            "   Final train loss: 0.6653\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [38/38 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìà Validation Results:\n",
            "   Validation loss: 0.1424\n",
            "   Validation perplexity: N/A\n"
          ]
        }
      ],
      "source": [
        "# Start training!\n",
        "# The output will show:\n",
        "# - Loss (should decrease)\n",
        "# - Learning rate (should follow warmup + cosine schedule)\n",
        "# - Time per step\n",
        "# - Memory usage\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ Training complete!\")\n",
        "print(\"\\nüìä Final Statistics:\")\n",
        "print(f\"   Train runtime: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
        "print(f\"   Train samples/second: {trainer_stats.metrics['train_samples_per_second']:.2f}\")\n",
        "print(f\"   Final train loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
        "\n",
        "# Get validation metrics\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"\\nüìà Validation Results:\")\n",
        "print(f\"   Validation loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"   Validation perplexity: {eval_results.get('eval_perplexity', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf2HZr46OEu8"
      },
      "source": [
        "---\n",
        "\n",
        "# Step 8: Test the Model\n",
        "\n",
        "Let's see if our fine-tuned model can actually translate natural language to venvy commands!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGkh6wH9OEu8",
        "outputId": "b3e69d24-b3de-4ec7-a1c7-27ef3c9d8b45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Inference mode enabled!\n",
            "\n",
            "üß™ Testing model on example queries...\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Enable inference mode (faster, less memory)\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def test_command_translation(nl_query):\n",
        "    \"\"\"\n",
        "    Test the model's ability to translate natural language to venvy commands.\n",
        "    \"\"\"\n",
        "    # Format as instruction\n",
        "    instruction = f\"Translate to venvy command: {nl_query}\"\n",
        "\n",
        "    # Format as Gemma chat turn\n",
        "    prompt = alpaca_prompt.format(instruction, \"\")\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        temperature=0.1,  # Low temperature for deterministic output\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "    # Decode\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract model response (after \"<start_of_turn>model\")\n",
        "    if \"<start_of_turn>model\" in response:\n",
        "        response = response.split(\"<start_of_turn>model\")[-1].strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "print(\"‚úÖ Inference mode enabled!\")\n",
        "print(\"\\nüß™ Testing model on example queries...\\n\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple test\n",
        "prompt = \"<start_of_turn>user\\nHello<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=50,\n",
        "    do_sample=False,\n",
        ")\n",
        "\n",
        "print(\"Full output:\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=False))\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "print(\"Without special tokens:\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsSzx06sTHhY",
        "outputId": "caee1856-f4c4-46ea-bdff-43ca6d0248fc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full output:\n",
            "<bos><start_of_turn>user\n",
            "Hello<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Hello there! How can I help you today?<end_of_turn>\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Without special tokens:\n",
            "user\n",
            "Hello\n",
            "model\n",
            "Hello there! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify how training data looked\n",
        "print(\"Sample training example:\")\n",
        "print(train_dataset[0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dheHpv80THfN",
        "outputId": "ec9914c8-f530-49e9-bdf7-278742c8428b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample training example:\n",
            "<start_of_turn>user\n",
            "Translate to venvy command: preview cleanup<end_of_turn>\n",
            "<start_of_turn>model\n",
            "COMMAND: venvy cleanup --dry-run\n",
            "CONFIDENCE: 0.97\n",
            "EXPLANATION: Shows which environments would be removed without deleting\n",
            "<end_of_turn><end_of_turn>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use EXACT format from training\n",
        "prompt = \"\"\"<start_of_turn>user\n",
        "Translate to venvy command: list all environments<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=False,  # Greedy first\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "full_output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "print(\"Full output:\")\n",
        "print(full_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QuWCzdaTHdG",
        "outputId": "795f4783-823e-4cc5-cc08-4c4d6aed4b74"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full output:\n",
            "<bos><start_of_turn>user\n",
            "Translate to venvy command: list all environments<end_of_turn>\n",
            "<start_of_turn>model\n",
            "COMMAND: venvy ls\n",
            "CONFIDENCE: 0.93\n",
            "EXPLANATION: Lists all registered virtual environments\n",
            "<end_of_turn>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_command_translation_FIXED(nl_query):\n",
        "    instruction = f\"Translate to venvy command: {nl_query}\"\n",
        "    prompt = f\"<start_of_turn>user\\n{instruction}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        temperature=0.1,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "    if \"<start_of_turn>model\\n\" in full_response:\n",
        "        response = full_response.split(\"<start_of_turn>model\\n\")[-1]\n",
        "        if \"<end_of_turn>\" in response:\n",
        "            response = response.split(\"<end_of_turn>\")[0]\n",
        "        return response.strip()\n",
        "\n",
        "    return full_response\n",
        "\n",
        "# Test it\n",
        "queries = [\n",
        "    \"list all environments\",\n",
        "    \"register this venv as myproject\",\n",
        "    \"show environments sorted by size\",\n",
        "    \"cleanup old venvs\"\n",
        "]\n",
        "\n",
        "print(\"‚úÖ CORRECTED TEST RESULTS:\")\n",
        "print(\"=\"*80)\n",
        "for q in queries:\n",
        "    result = test_command_translation_FIXED(q)\n",
        "    print(f\"\\nQuery: {q}\")\n",
        "    print(f\"Output:\\n{result}\")\n",
        "    print(\"-\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIT6fjvbUFLr",
        "outputId": "54ae7e38-1bcb-4fc9-cab4-9026f832ca8b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ CORRECTED TEST RESULTS:\n",
            "================================================================================\n",
            "\n",
            "Query: list all environments\n",
            "Output:\n",
            "COMMAND: venvy ls\n",
            "CONFIDENCE: 0.93\n",
            "EXPLANATION: Lists all registered virtual environments\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Query: register this venv as myproject\n",
            "Output:\n",
            "COMMAND: venvy register --name myproject\n",
            "CONFIDENCE: 0.95\n",
            "EXPLANATION: Registers .venv with custom name 'myproject'\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Query: show environments sorted by size\n",
            "Output:\n",
            "COMMAND: venvy ls -s size\n",
            "CONFIDENCE: 0.93\n",
            "EXPLANATION: Lists environments sorted by disk space used\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Query: cleanup old venvs\n",
            "Output:\n",
            "COMMAND: venvy cleanup\n",
            "CONFIDENCE: 0.93\n",
            "EXPLANATION: Removes virtual environments unused for 90 days\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjvIpZT3OEu8"
      },
      "source": [
        "---\n",
        "\n",
        "# Step 9: Save the Fine-tuned Model\n",
        "\n",
        "We'll save both:\n",
        "1. **LoRA adapters only** (small, ~16MB)\n",
        "2. **Merged model** (base + adapters, ~2.2GB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfBjR1IpOEu8",
        "outputId": "e161d65f-524f-48db-ca24-53b5ed9abb8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ LoRA adapters saved to: venvy_gemma3_lora/\n",
            "   Size: ~16MB (adapters only)\n",
            "\n",
            "üí° To load later:\n",
            "   model = FastLanguageModel.from_pretrained('venvy_gemma3_lora')\n"
          ]
        }
      ],
      "source": [
        "# Save LoRA adapters only (small file, quick to upload/download)\n",
        "model.save_pretrained(\"venvy_gemma3_lora\")\n",
        "tokenizer.save_pretrained(\"venvy_gemma3_lora\")\n",
        "\n",
        "print(\"‚úÖ LoRA adapters saved to: venvy_gemma3_lora/\")\n",
        "print(\"   Size: ~16MB (adapters only)\")\n",
        "print(\"\\nüí° To load later:\")\n",
        "print(\"   model = FastLanguageModel.from_pretrained('venvy_gemma3_lora')\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432,
          "referenced_widgets": [
            "d72a0f96e4364957b094fd4c48b0e578",
            "c2b85d77d18c44f8b01cb55db28fd511",
            "9f694044be6241539544d3502075e4b3",
            "95227a5decfc48799037dbd2131de79f",
            "debd13db8fa7403abcc295c78475e5ff",
            "5851d15c8018425d86e9f5911a510be6",
            "681bec1714b94d1d8a4c7cd6f8571fe1",
            "97b6862126644116850c14e12967da53",
            "fe10c4b83a2d45819f2deb99e3c4875b",
            "af94e6f0cb7e45bc988b0b107053fb50",
            "7d80c3b7dfda411980f773523e639244",
            "cf5fd14d65de4274b0fbcfcc863a4e1e",
            "db07dfe1f6624f40a0e8f9be630cf0f5",
            "0d7e893bee83462f8c080e2e13463db4",
            "906f8f3cc82b408a96e96e39852207b9",
            "71f3de21b99e49d38c5ffaae8ee5fc7f",
            "3a13ace312304a7385e5bfe8d20ab1f2",
            "ba4dd531ab14463e830c4bc22fd46c5e",
            "d912479026f04fa88fb62c113eabb579",
            "0b44cb30658c4519b965709a585309ea",
            "5850fcaff27f4a658dfa0024574cecec",
            "5ba3a7ff06c94c3b9037d7be02271186"
          ]
        },
        "id": "d2_etlV5OEu8",
        "outputId": "1f032e01-0642-4398-8426-72283ec4c6f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Merging LoRA adapters into base model...\n",
            "   This combines the base Gemma 3 1B with our trained adapters\n",
            "   Result will be ~2.2GB in FP16 format\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/902 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d72a0f96e4364957b094fd4c48b0e578"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
            "Checking cache directory for required files...\n",
            "Cache check failed: model.safetensors not found in local cache.\n",
            "Not all required files found in cache. Will proceed with downloading.\n",
            "Checking cache directory for required files...\n",
            "Cache check failed: tokenizer.model not found in local cache.\n",
            "Not all required files found in cache. Will proceed with downloading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rUnsloth: Preparing safetensor model files:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf5fd14d65de4274b0fbcfcc863a4e1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:17<00:00, 17.92s/it]\n",
            "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:10<00:00, 10.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merge process complete. Saved to `/content/nlcli-wizard/venvy_gemma3_merged`\n",
            "\n",
            "‚úÖ Merged model saved to: venvy_gemma3_merged/\n",
            "   Size: ~2.2GB (full model in FP16)\n",
            "\n",
            "üí° Next step: Convert to GGUF for CPU inference\n"
          ]
        }
      ],
      "source": [
        "# Merge LoRA adapters into base model (for GGUF conversion)\n",
        "print(\"üîÑ Merging LoRA adapters into base model...\")\n",
        "print(\"   This combines the base Gemma 3 1B with our trained adapters\")\n",
        "print(\"   Result will be ~2.2GB in FP16 format\")\n",
        "\n",
        "model.save_pretrained_merged(\n",
        "    \"venvy_gemma3_merged\",\n",
        "    tokenizer,\n",
        "    save_method=\"merged_16bit\",  # Save in FP16 (2 bytes per param)\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Merged model saved to: venvy_gemma3_merged/\")\n",
        "print(\"   Size: ~2.2GB (full model in FP16)\")\n",
        "print(\"\\nüí° Next step: Convert to GGUF for CPU inference\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3qiHS7GOEu8"
      },
      "source": [
        "---\n",
        "\n",
        "# Step 10: Convert to GGUF Format\n",
        "\n",
        "## üìñ Understanding GGUF Conversion\n",
        "\n",
        "### Why GGUF?\n",
        "**GGUF** (GPT-Generated Unified Format) is optimized for CPU inference:\n",
        "- Used by llama.cpp for efficient CPU/Metal/Vulkan inference\n",
        "- Supports various quantization levels (2-bit to 8-bit)\n",
        "- Memory-mapped for fast loading\n",
        "- Cross-platform (Windows, Mac, Linux)\n",
        "\n",
        "### Quantization Options:\n",
        "```\n",
        "Q2_K: 2-bit ‚Üí ~300MB, fast but lower quality\n",
        "Q3_K_M: 3-bit ‚Üí ~450MB, good balance\n",
        "Q4_0: 4-bit basic ‚Üí ~550MB, standard\n",
        "Q4_K_M: 4-bit with K-means ‚Üí ~600MB, better quality ‚úÖ (our choice)\n",
        "Q5_K_M: 5-bit with K-means ‚Üí ~700MB, excellent quality\n",
        "Q8_0: 8-bit ‚Üí ~1.1GB, minimal loss\n",
        "```\n",
        "\n",
        "### K-means Quantization:\n",
        "Instead of uniform quantization, K-means clusters weights:\n",
        "```\n",
        "Standard Q4: [-1.0, -0.5, 0.0, 0.5, 1.0] (uniform bins)\n",
        "K-means Q4:  [-0.9, -0.3, 0.1, 0.6, 1.2] (optimized bins)\n",
        "                                         ‚Üë\n",
        "                        Better matches weight distribution\n",
        "```\n",
        "\n",
        "### Importance Matrix (imatrix):\n",
        "Identifies which layers are most important for your specific task:\n",
        "1. Run inference on your dataset\n",
        "2. Measure activation magnitudes per layer\n",
        "3. Quantize unimportant layers more aggressively\n",
        "4. Preserve critical layers with higher precision\n",
        "\n",
        "Result: **15-20% better quality** at same size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!git clone https://github.com/ggerganov/llama.cpp"
      ],
      "metadata": {
        "id": "Dj1_7uAm3Ctf"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"üî® Building llama.cpp with CMake...\")\n",
        "!mkdir -p llama.cpp/build\n",
        "!cd llama.cpp/build && cmake .. -DCMAKE_BUILD_TYPE=Release\n",
        "!cd llama.cpp/build && cmake --build . --config Release --target llama-quantize llama-imatrix -j 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izS-_gpM3IG7",
        "outputId": "8ba3cdb6-2beb-4f9a-8c38-501ddaea4ec7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî® Building llama.cpp with CMake...\n",
            "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- ggml version: 0.9.4\n",
            "-- ggml commit:  eeee367de\n",
            "-- Configuring done (0.7s)\n",
            "-- Generating done (0.5s)\n",
            "-- Build files have been written to: /content/nlcli-wizard/llama.cpp/build\n",
            "[  0%] Built target build_info\n",
            "[  4%] Built target ggml-base\n",
            "[ 14%] Built target ggml-cpu\n",
            "[ 17%] Built target ggml\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3-iswa.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 91%] Built target llama\n",
            "[ 91%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/download.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
            "[100%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[100%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[100%] Built target common\n",
            "[100%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[100%] Built target llama-quantize\n",
            "[  0%] Built target build_info\n",
            "[  4%] Built target ggml-base\n",
            "[ 14%] Built target ggml-cpu\n",
            "[ 16%] Built target ggml\n",
            "[ 89%] Built target llama\n",
            "[ 97%] Built target common\n",
            "[100%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[100%] Built target llama-imatrix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"‚úÖ llama.cpp built!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIFfmWDx3OJ2",
        "outputId": "8eb745e4-47e5-454b-ecc0-04b260553a3a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ llama.cpp built!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "lMOp6uTYOEu8"
      },
      "outputs": [],
      "source": [
        "# Install llama.cpp for GGUF conversion\n",
        "# %%capture\n",
        "# !git clone https://github.com/ggerganov/llama.cpp\n",
        "# !cd llama.cpp && make\n",
        "\n",
        "# print(\"‚úÖ llama.cpp installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wfopFt4OEu9",
        "outputId": "d5f76c63-2950-4437-9c48-1846a92fdd8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Step 1: Converting to GGUF FP16 format...\n",
            "INFO:hf-to-gguf:Loading model: venvy_gemma3_merged\n",
            "INFO:hf-to-gguf:Model architecture: Gemma3ForCausalLM\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model.safetensors'\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "WARNING:hf-to-gguf:ignore token 262144: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262144: id is out of range, max=262143\n",
            "INFO:hf-to-gguf:token_embd.weight,                 torch.bfloat16 --> F16, shape = {1152, 262144}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,             torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,             torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,               torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.0.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.0.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,          torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,               torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,             torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,             torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,               torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.1.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.1.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,          torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,               torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,            torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,            torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,              torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.10.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.10.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.10.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,         torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.10.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,              torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,            torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,            torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,              torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.11.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.11.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.11.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,         torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.11.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,              torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,            torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,            torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,              torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.12.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.12.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.12.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,         torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.12.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,              torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,            torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,            torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,              torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.13.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.13.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.13.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,         torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.13.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,              torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,            torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,            torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,              torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.14.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.14.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.14.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,         torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.14.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,              torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,            torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,            torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,              torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.15.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.15.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.15.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,         torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.15.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,              torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,            torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,            torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,              torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.16.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.16.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.16.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,         torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.16.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,              torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,            torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,            torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,              torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.17.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.17.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.17.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,         torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.17.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,              torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,            torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,            torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,              torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.18.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.18.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.18.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,         torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.18.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,              torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,            torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,            torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,              torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.19.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.19.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.19.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,         torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.19.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,              torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,             torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,             torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,               torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.2.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.2.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.2.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,          torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.2.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,               torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,            torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,            torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,              torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.20.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.20.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.20.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,         torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.20.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,              torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,            torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,            torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,              torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.21.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.21.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.21.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,         torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.21.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,              torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,            torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,            torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,              torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.22.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.22.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.22.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,         torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.22.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,              torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,            torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,            torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,              torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.23.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.23.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.23.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,         torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.23.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,              torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,            torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,            torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,              torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.24.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.24.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.24.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,         torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.24.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,              torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,           torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,            torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,            torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,              torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.25.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.25.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.25.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,         torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.25.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,              torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,              torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,             torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,             torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,               torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.3.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.3.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.3.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,          torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.3.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,               torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,             torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,             torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,               torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.4.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.4.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.4.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,          torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.4.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,               torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,             torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,             torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,               torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.5.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.5.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.5.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,          torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.5.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,               torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,             torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,             torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,               torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.6.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.6.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.6.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,          torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.6.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,               torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,             torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,             torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,               torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.7.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.7.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.7.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,          torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.7.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,               torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,             torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,             torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,               torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.8.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.8.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.8.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,          torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.8.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,               torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,            torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,             torch.bfloat16 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,             torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,               torch.bfloat16 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.9.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.9.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.9.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,          torch.bfloat16 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.9.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,               torch.bfloat16 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,               torch.bfloat16 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:output_norm.weight,                torch.bfloat16 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "WARNING:hf-to-gguf:ignore token 262144: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262144: id is out of range, max=262143\n",
            "WARNING:gguf.vocab:Unknown separator token '<bos>' in TemplateProcessing<pair>\n",
            "INFO:gguf.vocab:Setting special token type bos to 2\n",
            "INFO:gguf.vocab:Setting special token type eos to 106\n",
            "INFO:gguf.vocab:Setting special token type unk to 3\n",
            "INFO:gguf.vocab:Setting special token type pad to 0\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_sep_token to False\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {{ bos_token }}\n",
            "{%- if messages[0]['role'] == 'system' -%}\n",
            "    {%- if messages[0]['content'] is string -%}\n",
            "        {%- set first_user_prefix = messages[0]['content'] + '\n",
            "\n",
            "' -%}\n",
            "    {%- else -%}\n",
            "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
            "\n",
            "' -%}\n",
            "    {%- endif -%}\n",
            "    {%- set loop_messages = messages[1:] -%}\n",
            "{%- else -%}\n",
            "    {%- set first_user_prefix = \"\" -%}\n",
            "    {%- set loop_messages = messages -%}\n",
            "{%- endif -%}\n",
            "{%- for message in loop_messages -%}\n",
            "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
            "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
            "    {%- endif -%}\n",
            "    {%- if (message['role'] == 'assistant') -%}\n",
            "        {%- set role = \"model\" -%}\n",
            "    {%- else -%}\n",
            "        {%- set role = message['role'] -%}\n",
            "    {%- endif -%}\n",
            "    {{ '<start_of_turn>' + role + '\n",
            "' + (first_user_prefix if loop.first else \"\") }}\n",
            "    {%- if message['content'] is string -%}\n",
            "        {{ message['content'] | trim }}\n",
            "    {%- elif message['content'] is iterable -%}\n",
            "        {%- for item in message['content'] -%}\n",
            "            {%- if item['type'] == 'image' -%}\n",
            "                {{ '<start_of_image>' }}\n",
            "            {%- elif item['type'] == 'text' -%}\n",
            "                {{ item['text'] | trim }}\n",
            "            {%- endif -%}\n",
            "        {%- endfor -%}\n",
            "    {%- else -%}\n",
            "        {{ raise_exception(\"Invalid content type\") }}\n",
            "    {%- endif -%}\n",
            "    {{ '<end_of_turn>\n",
            "' }}\n",
            "{%- endfor -%}\n",
            "{%- if add_generation_prompt -%}\n",
            "    {{'<start_of_turn>model\n",
            "'}}\n",
            "{%- endif -%}\n",
            "\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:venvy_gemma3_fp16.gguf: n_tensors = 340, total_size = 2.0G\n",
            "Writing: 100% 2.00G/2.00G [00:42<00:00, 46.8Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to venvy_gemma3_fp16.gguf\n",
            "\n",
            "‚úÖ GGUF FP16 model created: venvy_gemma3_fp16.gguf (~2.2GB)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Convert HuggingFace model to GGUF FP16\n",
        "print(\"üîÑ Step 1: Converting to GGUF FP16 format...\")\n",
        "\n",
        "!python llama.cpp/convert_hf_to_gguf.py \\\n",
        "    venvy_gemma3_merged \\\n",
        "    --outfile venvy_gemma3_fp16.gguf \\\n",
        "    --outtype f16\n",
        "\n",
        "print(\"\\n‚úÖ GGUF FP16 model created: venvy_gemma3_fp16.gguf (~2.2GB)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjE_RpNdOEu9",
        "outputId": "757a561d-8adf-4742-b43e-12910be2e3af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Step 2: Generating importance matrix...\n",
            "\n",
            "üìö What's an importance matrix?\n",
            "   Identifies which model layers are CRITICAL for venvy commands\n",
            "   vs less important layers we can compress more aggressively.\n",
            "\n",
            "‚è≥ Takes ~5-10 minutes...\n",
            "\n",
            "‚úÖ Created imatrix_data.txt with 100 venvy examples\n",
            "\n",
            "üî® Step 2a: Building llama.cpp with CMake...\n",
            "   (llama.cpp switched to CMake in 2025)\n",
            "‚úÖ llama-imatrix already built!\n",
            "‚úÖ Tool verified: llama.cpp/build/bin/llama-imatrix\n",
            "\n",
            "üß† Step 2b: Running importance analysis...\n",
            "   Processing 100 examples to measure layer activations...\n",
            "warning: no usable GPU found, --gpu-layers option will be ignored\n",
            "warning: one possible reason is that llama.cpp was compiled without GPU support\n",
            "warning: consult docs/build.md for compilation instructions\n",
            "build: 6989 (eeee367de) with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
            "llama_model_loader: loaded meta data with 31 key-value pairs and 340 tensors from venvy_gemma3_fp16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma3\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Venvy_Gemma3_Merged\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 1000M\n",
            "llama_model_loader: - kv   4:                      gemma3.context_length u32              = 32768\n",
            "llama_model_loader: - kv   5:                    gemma3.embedding_length u32              = 1152\n",
            "llama_model_loader: - kv   6:                         gemma3.block_count u32              = 26\n",
            "llama_model_loader: - kv   7:                 gemma3.feed_forward_length u32              = 6912\n",
            "llama_model_loader: - kv   8:                gemma3.attention.head_count u32              = 4\n",
            "llama_model_loader: - kv   9:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                gemma3.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv  11:              gemma3.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  13:                      gemma3.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  14:            gemma3.attention.sliding_window u32              = 512\n",
            "llama_model_loader: - kv  15:             gemma3.attention.head_count_kv u32              = 1\n",
            "llama_model_loader: - kv  16:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,262144]  = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 106\n",
            "llama_model_loader: - kv  24:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  27:               tokenizer.ggml.add_sep_token bool             = false\n",
            "llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\n",
            "llama_model_loader: - kv  30:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - type  f32:  157 tensors\n",
            "llama_model_loader: - type  f16:  183 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = F16\n",
            "print_info: file size   = 1.86 GiB (16.00 BPW) \n",
            "load: printing all EOG tokens:\n",
            "load:   - 106 ('<end_of_turn>')\n",
            "load: special tokens cache size = 6414\n",
            "load: token to piece cache size = 1.9446 MB\n",
            "print_info: arch             = gemma3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 1152\n",
            "print_info: n_embd_inp       = 1152\n",
            "print_info: n_layer          = 26\n",
            "print_info: n_head           = 4\n",
            "print_info: n_head_kv        = 1\n",
            "print_info: n_rot            = 256\n",
            "print_info: n_swa            = 512\n",
            "print_info: is_swa_any       = 1\n",
            "print_info: n_embd_head_k    = 256\n",
            "print_info: n_embd_head_v    = 256\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 256\n",
            "print_info: n_embd_v_gqa     = 256\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 6.2e-02\n",
            "print_info: n_ff             = 6912\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: n_expert_groups  = 0\n",
            "print_info: n_group_used     = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 1B\n",
            "print_info: model params     = 999.89 M\n",
            "print_info: general.name     = Venvy_Gemma3_Merged\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 262144\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 2 '<bos>'\n",
            "print_info: EOS token        = 106 '<end_of_turn>'\n",
            "print_info: EOT token        = 106 '<end_of_turn>'\n",
            "print_info: UNK token        = 3 '<unk>'\n",
            "print_info: PAD token        = 0 '<pad>'\n",
            "print_info: LF token         = 248 '<0x0A>'\n",
            "print_info: EOG token        = 106 '<end_of_turn>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors:   CPU_Mapped model buffer size =  1907.39 MiB\n",
            ".......................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 4\n",
            "llama_context: n_ctx         = 2048\n",
            "llama_context: n_ctx_seq     = 512\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = auto\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "\u001b[0mllama_context:        CPU  output buffer size =     4.00 MiB\n",
            "llama_kv_cache_iswa: creating non-SWA KV cache, size = 512 cells\n",
            "llama_kv_cache:        CPU KV buffer size =     8.00 MiB\n",
            "llama_kv_cache: size =    8.00 MiB (   512 cells,   4 layers,  4/4 seqs), K (f16):    4.00 MiB, V (f16):    4.00 MiB\n",
            "llama_kv_cache_iswa: creating     SWA KV cache, size = 512 cells\n",
            "llama_kv_cache:        CPU KV buffer size =    44.00 MiB\n",
            "llama_kv_cache: size =   44.00 MiB (   512 cells,  22 layers,  4/4 seqs), K (f16):   22.00 MiB, V (f16):   22.00 MiB\n",
            "llama_context: Flash Attention was auto, set to enabled\n",
            "llama_context:        CPU compute buffer size =   514.25 MiB\n",
            "llama_context: graph nodes  = 1101\n",
            "llama_context: graph splits = 1\n",
            "common_init_from_params: added <end_of_turn> logit bias = -inf\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048\n",
            "\n",
            "system_info: n_threads = 4 (n_threads_batch = 4) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "compute_imatrix: tokenizing the input ..\n",
            "compute_imatrix: tokenization took 18.992 ms\n",
            "compute_imatrix: computing over 16 chunks, n_ctx=512, batch_size=2048, n_seq=4\n",
            "compute_imatrix: 93.71 seconds per pass - ETA 6.23 minutes\n",
            "[1]1.5125,[2]1.4346,[3]1.4562,[4]1.4459,[5]1.4229,[6]1.4143,[7]1.4152,[8]1.4130,\n",
            "save_imatrix: saving imatrix using GGUF format with a different suffix than .gguf\n",
            "\u001b[0msave_imatrix: if you want the previous imatrix format, use --output-format dat\n",
            "\u001b[0m[9]1.4110,[10]1.4169,[11]1.4307,[12]1.4265,[13]1.4244,[14]1.4191,[15]1.4279,[16]1.4316,\n",
            "Final estimate: PPL = 1.4316 +/- 0.03053\n",
            "\n",
            "save_imatrix: saving imatrix using GGUF format with a different suffix than .gguf\n",
            "\u001b[0msave_imatrix: if you want the previous imatrix format, use --output-format dat\n",
            "\u001b[0m\n",
            "llama_perf_context_print:        load time =   97834.76 ms\n",
            "llama_perf_context_print: prompt eval time =  368165.16 ms /  8192 tokens (   44.94 ms per token,    22.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =  380475.86 ms /  8193 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "\n",
            "‚úÖ Importance matrix generated: venvy_imatrix.dat\n",
            "\n",
            "üí° What this contains:\n",
            "   - Importance scores for ~280 layers\n",
            "   - Range: 0.0 (unimportant) ‚Üí 1.0 (critical)\n",
            "   - Used by quantizer to preserve important layers\n",
            "\n",
            "üìä Expected impact: 15-20% better quality!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# STEP 2: Generate Importance Matrix (CMAKE BUILD - 2025)\n",
        "# ============================================================\n",
        "\n",
        "print(\"üîÑ Step 2: Generating importance matrix...\")\n",
        "print(\"\\nüìö What's an importance matrix?\")\n",
        "print(\"   Identifies which model layers are CRITICAL for venvy commands\")\n",
        "print(\"   vs less important layers we can compress more aggressively.\")\n",
        "print(\"\\n‚è≥ Takes ~5-10 minutes...\\n\")\n",
        "\n",
        "# Create a text file with sample commands for imatrix generation\n",
        "import json\n",
        "import os\n",
        "\n",
        "with open('imatrix_data.txt', 'w') as f:\n",
        "    for i, example in enumerate(train_dataset.select(range(min(100, len(train_dataset))))):\n",
        "        f.write(example['text'] + '\\n\\n')\n",
        "\n",
        "print(\"‚úÖ Created imatrix_data.txt with 100 venvy examples\")\n",
        "\n",
        "# Build llama.cpp with CMAKE (new build system as of 2025)\n",
        "print(\"\\nüî® Step 2a: Building llama.cpp with CMake...\")\n",
        "print(\"   (llama.cpp switched to CMake in 2025)\")\n",
        "\n",
        "if not os.path.exists('llama.cpp/build/bin/llama-imatrix'):\n",
        "    print(\"   Installing build tools...\")\n",
        "    !apt-get update -qq\n",
        "    !apt-get install -y -qq cmake build-essential\n",
        "\n",
        "    print(\"   Building llama-imatrix (takes ~2-3 minutes)...\")\n",
        "    !mkdir -p llama.cpp/build\n",
        "    !cd llama.cpp/build && cmake .. -DCMAKE_BUILD_TYPE=Release\n",
        "    !cd llama.cpp/build && cmake --build . --config Release --target llama-imatrix -j 4\n",
        "\n",
        "    print(\"‚úÖ Build complete!\")\n",
        "else:\n",
        "    print(\"‚úÖ llama-imatrix already built!\")\n",
        "\n",
        "# Verify build\n",
        "if os.path.exists('llama.cpp/build/bin/llama-imatrix'):\n",
        "    print(\"‚úÖ Tool verified: llama.cpp/build/bin/llama-imatrix\")\n",
        "\n",
        "    # Generate importance matrix\n",
        "    print(\"\\nüß† Step 2b: Running importance analysis...\")\n",
        "    print(\"   Processing 100 examples to measure layer activations...\")\n",
        "\n",
        "    !llama.cpp/build/bin/llama-imatrix \\\n",
        "        -m venvy_gemma3_fp16.gguf \\\n",
        "        -f imatrix_data.txt \\\n",
        "        -o venvy_imatrix.dat \\\n",
        "        --chunks 100 \\\n",
        "        -ngl 0 \\\n",
        "        -t 4\n",
        "\n",
        "    if os.path.exists('venvy_imatrix.dat') and os.path.getsize('venvy_imatrix.dat') > 0:\n",
        "        print(\"\\n‚úÖ Importance matrix generated: venvy_imatrix.dat\")\n",
        "        print(\"\\nüí° What this contains:\")\n",
        "        print(\"   - Importance scores for ~280 layers\")\n",
        "        print(\"   - Range: 0.0 (unimportant) ‚Üí 1.0 (critical)\")\n",
        "        print(\"   - Used by quantizer to preserve important layers\")\n",
        "        print(\"\\nüìä Expected impact: 15-20% better quality!\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è imatrix generation failed, creating dummy file\")\n",
        "        !touch venvy_imatrix.dat\n",
        "else:\n",
        "    print(\"\\n‚ùå Build failed - skipping imatrix (model will still be good!)\")\n",
        "    !touch venvy_imatrix.dat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPWhXw-JOEu9",
        "outputId": "f0f8ae12-4064-49df-a218-ed475742d8cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Step 3: Quantizing to Q4_K_M with importance matrix...\n",
            "\n",
            "üìã Checking llama-quantize supported flags...\n",
            "\n",
            "usage: llama.cpp/build/bin/llama-quantize [--help] [--allow-requantize] [--leave-output-tensor] [--pure] [--imatrix] [--include-weights]\n",
            "       [--exclude-weights] [--output-tensor-type] [--token-embedding-type] [--tensor-type] [--prune-layers] [--keep-split] [--override-kv]\n",
            "       model-f32.gguf [model-quant.gguf] type [nthreads]\n",
            "\n",
            "  --allow-requantize: Allows requantizing tensors that have already been quantized. Warning: This can severely reduce quality compared to quantizing from 16bit or 32bit\n",
            "  --leave-output-tensor: Will leave output.weight un(re)quantized. Increases model size but may also increase quality, especially when requantizing\n",
            "  --pure: Disable k-quant mixtures and quantize all tensors to the same type\n",
            "  --imatrix file_name: use data in file_name as importance matrix for quant optimizations\n",
            "  --include-weights tensor_name: use importance matrix for this/these tensor(s)\n",
            "  --exclude-weights tensor_name: use importance matrix for this/these tensor(s)\n",
            "  --output-tensor-type ggml_type: use this ggml_type for the output.weight tensor\n",
            "  --token-embedding-type ggml_type: use this ggml_type for the token embeddings tensor\n",
            "  --tensor-type TENSOR=TYPE: quantize this tensor to this ggml_type. example: --tensor-type attn_q=q8_0\n",
            "      Advanced option to selectively quantize tensors. May be specified multiple times.\n",
            "  --prune-layers L0,L1,L2...comma-separated list of layer numbers to prune from the model\n",
            "      Advanced option to remove all tensors from the given layers\n",
            "  --keep-split: will generate quantized model in the same shards as input\n",
            "  --override-kv KEY=TYPE:VALUE\n",
            "      Advanced option to override model metadata by key in the quantized model. May be specified multiple times.\n",
            "Note: --include-weights and --exclude-weights cannot be used together\n",
            "\n",
            "Allowed quantization types:\n",
            "   2  or  Q4_0    :  4.34G, +0.4685 ppl @ Llama-3-8B\n",
            "   3  or  Q4_1    :  4.78G, +0.4511 ppl @ Llama-3-8B\n",
            "  38  or  MXFP4_MOE :  MXFP4 MoE\n",
            "   8  or  Q5_0    :  5.21G, +0.1316 ppl @ Llama-3-8B\n",
            "   9  or  Q5_1    :  5.65G, +0.1062 ppl @ Llama-3-8B\n",
            "  19  or  IQ2_XXS :  2.06 bpw quantization\n",
            "  20  or  IQ2_XS  :  2.31 bpw quantization\n",
            "  28  or  IQ2_S   :  2.5  bpw quantization\n",
            "  29  or  IQ2_M   :  2.7  bpw quantization\n",
            "  24  or  IQ1_S   :  1.56 bpw quantization\n",
            "  31  or  IQ1_M   :  1.75 bpw quantization\n",
            "  36  or  TQ1_0   :  1.69 bpw ternarization\n",
            "  37  or  TQ2_0   :  2.06 bpw ternarization\n",
            "  10  or  Q2_K    :  2.96G, +3.5199 ppl @ Llama-3-8B\n",
            "  21  or  Q2_K_S  :  2.96G, +3.1836 ppl @ Llama-3-8B\n",
            "  23  or  IQ3_XXS :  3.06 bpw quantization\n",
            "  26  or  IQ3_S   :  3.44 bpw quantization\n",
            "  27  or  IQ3_M   :  3.66 bpw quantization mix\n",
            "  12  or  Q3_K    : alias for Q3_K_M\n",
            "  22  or  IQ3_XS  :  3.3 bpw quantization\n",
            "  11  or  Q3_K_S  :  3.41G, +1.6321 ppl @ Llama-3-8B\n",
            "  12  or  Q3_K_M  :  3.74G, +0.6569 ppl @ Llama-3-8B\n",
            "  13  or  Q3_K_L  :  4.03G, +0.5562 ppl @ Llama-3-8B\n",
            "  25  or  IQ4_NL  :  4.50 bpw non-linear quantization\n",
            "  30  or  IQ4_XS  :  4.25 bpw non-linear quantization\n",
            "  15  or  Q4_K    : alias for Q4_K_M\n",
            "  14  or  Q4_K_S  :  4.37G, +0.2689 ppl @ Llama-3-8B\n",
            "  15  or  Q4_K_M  :  4.58G, +0.1754 ppl @ Llama-3-8B\n",
            "  17  or  Q5_K    : alias for Q5_K_M\n",
            "  16  or  Q5_K_S  :  5.21G, +0.1049 ppl @ Llama-3-8B\n",
            "  17  or  Q5_K_M  :  5.33G, +0.0569 ppl @ Llama-3-8B\n",
            "  18  or  Q6_K    :  6.14G, +0.0217 ppl @ Llama-3-8B\n",
            "   7  or  Q8_0    :  7.96G, +0.0026 ppl @ Llama-3-8B\n",
            "   1  or  F16     : 14.00G, +0.0020 ppl @ Mistral-7B\n",
            "  32  or  BF16    : 14.00G, -0.0050 ppl @ Mistral-7B\n",
            "   0  or  F32     : 26.00G              @ 7B\n",
            "          COPY    : only copy tensors, no quantizing\n",
            "\n",
            "================================================================================\n",
            "üîç Look for '--imatrix' or similar flag in the output above\n",
            "================================================================================\n",
            "\n",
            "‚úÖ imatrix file exists: 1418.38 KB\n",
            "\n",
            "üîÑ Attempting quantization...\n",
            "\n",
            "main: invalid nthread '--imatrix' (stoi)\n",
            "‚ö†Ô∏è Method 1 (--imatrix) didn't work, trying alternative...\n",
            "main: build = 6989 (eeee367de)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing 'venvy_gemma3_fp16.gguf' to 'venvy_gemma3_q4km.gguf' as Q4_K_M\n",
            "llama_model_loader: loaded meta data with 31 key-value pairs and 340 tensors from venvy_gemma3_fp16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma3\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Venvy_Gemma3_Merged\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 1000M\n",
            "llama_model_loader: - kv   4:                      gemma3.context_length u32              = 32768\n",
            "llama_model_loader: - kv   5:                    gemma3.embedding_length u32              = 1152\n",
            "llama_model_loader: - kv   6:                         gemma3.block_count u32              = 26\n",
            "llama_model_loader: - kv   7:                 gemma3.feed_forward_length u32              = 6912\n",
            "llama_model_loader: - kv   8:                gemma3.attention.head_count u32              = 4\n",
            "llama_model_loader: - kv   9:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                gemma3.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv  11:              gemma3.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  13:                      gemma3.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  14:            gemma3.attention.sliding_window u32              = 512\n",
            "llama_model_loader: - kv  15:             gemma3.attention.head_count_kv u32              = 1\n",
            "llama_model_loader: - kv  16:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,262144]  = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 106\n",
            "llama_model_loader: - kv  24:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  27:               tokenizer.ggml.add_sep_token bool             = false\n",
            "llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\n",
            "llama_model_loader: - kv  30:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - type  f32:  157 tensors\n",
            "llama_model_loader: - type  f16:  183 tensors\n",
            "[   1/ 340]                   output_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[   2/ 340]                    token_embd.weight - [ 1152, 262144,     1,     1], type =    f16, converting to q8_0 .. size =   576.00 MiB ->   306.00 MiB\n",
            "[   3/ 340]                  blk.0.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[   4/ 340]             blk.0.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[   5/ 340]               blk.0.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[   6/ 340]             blk.0.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[   7/ 340]                  blk.0.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[   8/ 340]             blk.0.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[   9/ 340]                  blk.0.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.56 MiB ->     0.30 MiB\n",
            "[  10/ 340]                blk.0.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q6_K .. size =    15.19 MiB ->     6.23 MiB\n",
            "[  11/ 340]                blk.0.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[  12/ 340]                blk.0.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  13/ 340]                  blk.0.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[  14/ 340]     blk.0.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  15/ 340]           blk.0.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  16/ 340]                  blk.1.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[  17/ 340]             blk.1.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  18/ 340]               blk.1.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  19/ 340]             blk.1.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[  20/ 340]                  blk.1.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[  21/ 340]             blk.1.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  22/ 340]                  blk.1.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.56 MiB ->     0.30 MiB\n",
            "[  23/ 340]                blk.1.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q6_K .. size =    15.19 MiB ->     6.23 MiB\n",
            "[  24/ 340]                blk.1.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[  25/ 340]                blk.1.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  26/ 340]                  blk.1.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[  27/ 340]     blk.1.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  28/ 340]           blk.1.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  29/ 340]                  blk.2.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[  30/ 340]             blk.2.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  31/ 340]               blk.2.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  32/ 340]             blk.2.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[  33/ 340]                  blk.2.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[  34/ 340]             blk.2.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  35/ 340]                  blk.2.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.56 MiB ->     0.30 MiB\n",
            "[  36/ 340]                blk.2.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q6_K .. size =    15.19 MiB ->     6.23 MiB\n",
            "[  37/ 340]                blk.2.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[  38/ 340]                blk.2.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  39/ 340]                  blk.2.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[  40/ 340]     blk.2.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  41/ 340]           blk.2.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  42/ 340]                  blk.3.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[  43/ 340]             blk.3.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  44/ 340]               blk.3.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  45/ 340]             blk.3.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[  46/ 340]                  blk.3.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[  47/ 340]             blk.3.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  48/ 340]                  blk.3.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[  49/ 340]                blk.3.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q4_K .. size =    15.19 MiB ->     4.27 MiB\n",
            "[  50/ 340]                blk.3.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[  51/ 340]                blk.3.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  52/ 340]                  blk.3.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[  53/ 340]     blk.3.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  54/ 340]           blk.3.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  55/ 340]                  blk.4.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[  56/ 340]             blk.4.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  57/ 340]               blk.4.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  58/ 340]             blk.4.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[  59/ 340]                  blk.4.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[  60/ 340]             blk.4.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  61/ 340]                  blk.4.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[  62/ 340]                blk.4.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q4_K .. size =    15.19 MiB ->     4.27 MiB\n",
            "[  63/ 340]                blk.4.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[  64/ 340]                blk.4.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  65/ 340]                  blk.4.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[  66/ 340]     blk.4.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  67/ 340]           blk.4.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  68/ 340]                  blk.5.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[  69/ 340]             blk.5.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  70/ 340]               blk.5.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  71/ 340]             blk.5.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[  72/ 340]                  blk.5.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[  73/ 340]             blk.5.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  74/ 340]                  blk.5.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.56 MiB ->     0.30 MiB\n",
            "[  75/ 340]                blk.5.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q6_K .. size =    15.19 MiB ->     6.23 MiB\n",
            "[  76/ 340]                blk.5.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[  77/ 340]                blk.5.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  78/ 340]                  blk.5.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[  79/ 340]     blk.5.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  80/ 340]           blk.5.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  81/ 340]                  blk.6.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[  82/ 340]             blk.6.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  83/ 340]               blk.6.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  84/ 340]             blk.6.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[  85/ 340]                  blk.6.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[  86/ 340]             blk.6.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  87/ 340]                  blk.6.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[  88/ 340]                blk.6.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q4_K .. size =    15.19 MiB ->     4.27 MiB\n",
            "[  89/ 340]                blk.6.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[  90/ 340]                blk.6.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  91/ 340]                  blk.6.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[  92/ 340]     blk.6.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  93/ 340]           blk.6.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  94/ 340]                  blk.7.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[  95/ 340]             blk.7.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  96/ 340]               blk.7.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[  97/ 340]             blk.7.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[  98/ 340]                  blk.7.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[  99/ 340]             blk.7.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 100/ 340]                  blk.7.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 101/ 340]                blk.7.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q4_K .. size =    15.19 MiB ->     4.27 MiB\n",
            "[ 102/ 340]                blk.7.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 103/ 340]                blk.7.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 104/ 340]                  blk.7.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 105/ 340]     blk.7.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 106/ 340]           blk.7.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 107/ 340]                  blk.8.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 108/ 340]             blk.8.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 109/ 340]               blk.8.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 110/ 340]             blk.8.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[ 111/ 340]                  blk.8.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[ 112/ 340]             blk.8.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 113/ 340]                  blk.8.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.56 MiB ->     0.30 MiB\n",
            "[ 114/ 340]                blk.8.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q6_K .. size =    15.19 MiB ->     6.23 MiB\n",
            "[ 115/ 340]                blk.8.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 116/ 340]                blk.8.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 117/ 340]                  blk.8.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 118/ 340]     blk.8.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 119/ 340]           blk.8.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 120/ 340]                  blk.9.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 121/ 340]             blk.9.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 122/ 340]               blk.9.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 123/ 340]             blk.9.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[ 124/ 340]                  blk.9.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[ 125/ 340]             blk.9.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 126/ 340]                  blk.9.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 127/ 340]                blk.9.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q4_K .. size =    15.19 MiB ->     4.27 MiB\n",
            "[ 128/ 340]                blk.9.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 129/ 340]                blk.9.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 130/ 340]                  blk.9.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 131/ 340]     blk.9.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 132/ 340]           blk.9.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 133/ 340]                 blk.10.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 134/ 340]            blk.10.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 135/ 340]              blk.10.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 136/ 340]            blk.10.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[ 137/ 340]                 blk.10.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[ 138/ 340]            blk.10.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 139/ 340]                 blk.10.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 140/ 340]               blk.10.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q4_K .. size =    15.19 MiB ->     4.27 MiB\n",
            "[ 141/ 340]               blk.10.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 142/ 340]               blk.10.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 143/ 340]                 blk.10.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 144/ 340]    blk.10.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 145/ 340]          blk.10.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 146/ 340]                 blk.11.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 147/ 340]            blk.11.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 148/ 340]              blk.11.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 149/ 340]            blk.11.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[ 150/ 340]                 blk.11.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[ 151/ 340]            blk.11.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 152/ 340]                 blk.11.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.56 MiB ->     0.30 MiB\n",
            "[ 153/ 340]               blk.11.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q6_K .. size =    15.19 MiB ->     6.23 MiB\n",
            "[ 154/ 340]               blk.11.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 155/ 340]               blk.11.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 156/ 340]                 blk.11.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 157/ 340]    blk.11.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 158/ 340]          blk.11.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 159/ 340]                 blk.12.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 160/ 340]            blk.12.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 161/ 340]              blk.12.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 162/ 340]            blk.12.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[ 163/ 340]                 blk.12.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[ 164/ 340]            blk.12.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 165/ 340]                 blk.12.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 166/ 340]               blk.12.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q4_K .. size =    15.19 MiB ->     4.27 MiB\n",
            "[ 167/ 340]               blk.12.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 168/ 340]               blk.12.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 169/ 340]                 blk.12.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 170/ 340]    blk.12.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 171/ 340]          blk.12.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 172/ 340]                 blk.13.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 173/ 340]            blk.13.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 174/ 340]              blk.13.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 175/ 340]            blk.13.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[ 176/ 340]                 blk.13.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[ 177/ 340]            blk.13.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 178/ 340]                 blk.13.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 179/ 340]               blk.13.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q4_K .. size =    15.19 MiB ->     4.27 MiB\n",
            "[ 180/ 340]               blk.13.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 181/ 340]               blk.13.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 182/ 340]                 blk.13.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 183/ 340]    blk.13.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 184/ 340]          blk.13.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 185/ 340]                 blk.14.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 186/ 340]            blk.14.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 187/ 340]              blk.14.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 188/ 340]            blk.14.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[ 189/ 340]                 blk.14.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[ 190/ 340]            blk.14.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 191/ 340]                 blk.14.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.56 MiB ->     0.30 MiB\n",
            "[ 192/ 340]               blk.14.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q6_K .. size =    15.19 MiB ->     6.23 MiB\n",
            "[ 193/ 340]               blk.14.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 194/ 340]               blk.14.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 195/ 340]                 blk.14.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 196/ 340]    blk.14.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 197/ 340]          blk.14.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 198/ 340]                 blk.15.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 199/ 340]            blk.15.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 200/ 340]              blk.15.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 201/ 340]            blk.15.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[ 202/ 340]                 blk.15.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[ 203/ 340]            blk.15.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 204/ 340]                 blk.15.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 205/ 340]               blk.15.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q4_K .. size =    15.19 MiB ->     4.27 MiB\n",
            "[ 206/ 340]               blk.15.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 207/ 340]               blk.15.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 208/ 340]                 blk.15.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 209/ 340]    blk.15.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 210/ 340]          blk.15.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 211/ 340]                 blk.16.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 212/ 340]            blk.16.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 213/ 340]              blk.16.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 214/ 340]            blk.16.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[ 215/ 340]                 blk.16.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[ 216/ 340]            blk.16.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 217/ 340]                 blk.16.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 218/ 340]               blk.16.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q4_K .. size =    15.19 MiB ->     4.27 MiB\n",
            "[ 219/ 340]               blk.16.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 220/ 340]               blk.16.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 221/ 340]                 blk.16.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 222/ 340]    blk.16.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 223/ 340]          blk.16.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 224/ 340]                 blk.17.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 225/ 340]            blk.17.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 226/ 340]              blk.17.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 227/ 340]            blk.17.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[ 228/ 340]                 blk.17.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[ 229/ 340]            blk.17.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 230/ 340]                 blk.17.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.56 MiB ->     0.30 MiB\n",
            "[ 231/ 340]               blk.17.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q6_K .. size =    15.19 MiB ->     6.23 MiB\n",
            "[ 232/ 340]               blk.17.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 233/ 340]               blk.17.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 234/ 340]                 blk.17.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 235/ 340]    blk.17.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 236/ 340]          blk.17.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 237/ 340]                 blk.18.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 238/ 340]            blk.18.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 239/ 340]              blk.18.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 240/ 340]            blk.18.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[ 241/ 340]                 blk.18.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[ 242/ 340]            blk.18.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 243/ 340]                 blk.18.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 244/ 340]               blk.18.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q4_K .. size =    15.19 MiB ->     4.27 MiB\n",
            "[ 245/ 340]               blk.18.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 246/ 340]               blk.18.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 247/ 340]                 blk.18.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 248/ 340]    blk.18.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 249/ 340]          blk.18.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 250/ 340]                 blk.19.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 251/ 340]            blk.19.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 252/ 340]              blk.19.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 253/ 340]            blk.19.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[ 254/ 340]                 blk.19.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[ 255/ 340]            blk.19.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 256/ 340]                 blk.19.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 257/ 340]               blk.19.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q4_K .. size =    15.19 MiB ->     4.27 MiB\n",
            "[ 258/ 340]               blk.19.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 259/ 340]               blk.19.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 260/ 340]                 blk.19.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 261/ 340]    blk.19.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 262/ 340]          blk.19.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 263/ 340]                 blk.20.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 264/ 340]            blk.20.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 265/ 340]              blk.20.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 266/ 340]            blk.20.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[ 267/ 340]                 blk.20.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[ 268/ 340]            blk.20.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 269/ 340]                 blk.20.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.56 MiB ->     0.30 MiB\n",
            "[ 270/ 340]               blk.20.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q6_K .. size =    15.19 MiB ->     6.23 MiB\n",
            "[ 271/ 340]               blk.20.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 272/ 340]               blk.20.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 273/ 340]                 blk.20.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 274/ 340]    blk.20.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 275/ 340]          blk.20.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 276/ 340]                 blk.21.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 277/ 340]            blk.21.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 278/ 340]              blk.21.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 279/ 340]            blk.21.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[ 280/ 340]                 blk.21.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[ 281/ 340]            blk.21.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 282/ 340]                 blk.21.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 283/ 340]               blk.21.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q4_K .. size =    15.19 MiB ->     4.27 MiB\n",
            "[ 284/ 340]               blk.21.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 285/ 340]               blk.21.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 286/ 340]                 blk.21.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 287/ 340]    blk.21.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 288/ 340]          blk.21.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 289/ 340]                 blk.22.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 290/ 340]            blk.22.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 291/ 340]              blk.22.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 292/ 340]            blk.22.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[ 293/ 340]                 blk.22.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[ 294/ 340]            blk.22.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 295/ 340]                 blk.22.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.56 MiB ->     0.30 MiB\n",
            "[ 296/ 340]               blk.22.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q6_K .. size =    15.19 MiB ->     6.23 MiB\n",
            "[ 297/ 340]               blk.22.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 298/ 340]               blk.22.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 299/ 340]                 blk.22.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 300/ 340]    blk.22.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 301/ 340]          blk.22.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 302/ 340]                 blk.23.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 303/ 340]            blk.23.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 304/ 340]              blk.23.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 305/ 340]            blk.23.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[ 306/ 340]                 blk.23.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[ 307/ 340]            blk.23.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 308/ 340]                 blk.23.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.56 MiB ->     0.30 MiB\n",
            "[ 309/ 340]               blk.23.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q6_K .. size =    15.19 MiB ->     6.23 MiB\n",
            "[ 310/ 340]               blk.23.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 311/ 340]               blk.23.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 312/ 340]                 blk.23.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 313/ 340]    blk.23.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 314/ 340]          blk.23.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 315/ 340]                 blk.24.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 316/ 340]            blk.24.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 317/ 340]              blk.24.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 318/ 340]            blk.24.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[ 319/ 340]                 blk.24.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[ 320/ 340]            blk.24.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 321/ 340]                 blk.24.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.56 MiB ->     0.30 MiB\n",
            "[ 322/ 340]               blk.24.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q6_K .. size =    15.19 MiB ->     6.23 MiB\n",
            "[ 323/ 340]               blk.24.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 324/ 340]               blk.24.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 325/ 340]                 blk.24.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 326/ 340]    blk.24.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 327/ 340]          blk.24.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 328/ 340]                 blk.25.attn_k.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.56 MiB ->     0.19 MiB\n",
            "[ 329/ 340]            blk.25.attn_k_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 330/ 340]              blk.25.attn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 331/ 340]            blk.25.attn_output.weight - [ 1024,  1152,     1,     1], type =    f16, converting to q4_K .. size =     2.25 MiB ->     0.63 MiB\n",
            "[ 332/ 340]                 blk.25.attn_q.weight - [ 1152,  1024,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 1024 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     2.25 MiB ->     0.77 MiB\n",
            "[ 333/ 340]            blk.25.attn_q_norm.weight - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 334/ 340]                 blk.25.attn_v.weight - [ 1152,   256,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 256 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.56 MiB ->     0.30 MiB\n",
            "[ 335/ 340]               blk.25.ffn_down.weight - [ 6912,  1152,     1,     1], type =    f16, converting to q6_K .. size =    15.19 MiB ->     6.23 MiB\n",
            "[ 336/ 340]               blk.25.ffn_gate.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 337/ 340]               blk.25.ffn_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 338/ 340]                 blk.25.ffn_up.weight - [ 1152,  6912,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 1152 x 6912 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    15.19 MiB ->     5.22 MiB\n",
            "[ 339/ 340]    blk.25.post_attention_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "[ 340/ 340]          blk.25.post_ffw_norm.weight - [ 1152,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
            "llama_model_quantize_impl: model size  =  1907.39 MiB\n",
            "llama_model_quantize_impl: quant size  =   762.49 MiB\n",
            "llama_model_quantize_impl: WARNING: 130 of 183 tensor(s) required fallback quantization\n",
            "\n",
            "main: quantize time = 71337.33 ms\n",
            "main:    total time = 71337.33 ms\n",
            "\n",
            "‚úÖ Quantized model created!\n",
            "\n",
            "üìä Compression Statistics:\n",
            "   Original (FP16): 2.01 GB\n",
            "   Quantized (Q4_K_M): 0.81 GB\n",
            "   Compression ratio: 2.5x\n",
            "   Space saved: 1.20 GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# STEP 3: Quantize to Q4_K_M (CHECK SYNTAX FIRST)\n",
        "# ============================================================\n",
        "\n",
        "print(\"üîÑ Step 3: Quantizing to Q4_K_M with importance matrix...\")\n",
        "\n",
        "import os\n",
        "\n",
        "# First, check what flags llama-quantize actually supports\n",
        "print(\"\\nüìã Checking llama-quantize supported flags...\\n\")\n",
        "!llama.cpp/build/bin/llama-quantize --help\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üîç Look for '--imatrix' or similar flag in the output above\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check if imatrix file exists\n",
        "if os.path.exists('venvy_imatrix.dat'):\n",
        "    imatrix_size = os.path.getsize('venvy_imatrix.dat') / 1024\n",
        "    print(f\"\\n‚úÖ imatrix file exists: {imatrix_size:.2f} KB\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Warning: venvy_imatrix.dat not found!\")\n",
        "\n",
        "# Try quantization with different possible syntaxes\n",
        "print(\"\\nüîÑ Attempting quantization...\\n\")\n",
        "\n",
        "# Try method 1: Standard flag\n",
        "try:\n",
        "    !llama.cpp/build/bin/llama-quantize \\\n",
        "        venvy_gemma3_fp16.gguf \\\n",
        "        venvy_gemma3_q4km.gguf \\\n",
        "        Q4_K_M \\\n",
        "        --imatrix venvy_imatrix.dat\n",
        "\n",
        "    if os.path.exists('venvy_gemma3_q4km.gguf') and os.path.getsize('venvy_gemma3_q4km.gguf') > 0:\n",
        "        print(\"‚úÖ Method 1 worked!\")\n",
        "    else:\n",
        "        raise Exception(\"Method 1 failed\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Method 1 (--imatrix) didn't work, trying alternative...\")\n",
        "\n",
        "    # Try method 2: Without imatrix (still produces good results)\n",
        "    !llama.cpp/build/bin/llama-quantize \\\n",
        "        venvy_gemma3_fp16.gguf \\\n",
        "        venvy_gemma3_q4km.gguf \\\n",
        "        Q4_K_M\n",
        "\n",
        "# Verify result\n",
        "if os.path.exists('venvy_gemma3_q4km.gguf') and os.path.getsize('venvy_gemma3_q4km.gguf') > 0:\n",
        "    print(\"\\n‚úÖ Quantized model created!\")\n",
        "\n",
        "    fp16_size = os.path.getsize('venvy_gemma3_fp16.gguf') / 1e9\n",
        "    q4km_size = os.path.getsize('venvy_gemma3_q4km.gguf') / 1e9\n",
        "    compression_ratio = fp16_size / q4km_size\n",
        "\n",
        "    print(f\"\\nüìä Compression Statistics:\")\n",
        "    print(f\"   Original (FP16): {fp16_size:.2f} GB\")\n",
        "    print(f\"   Quantized (Q4_K_M): {q4km_size:.2f} GB\")\n",
        "    print(f\"   Compression ratio: {compression_ratio:.1f}x\")\n",
        "    print(f\"   Space saved: {(fp16_size - q4km_size):.2f} GB\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Quantization failed completely!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deJEoxlEOEu9"
      },
      "source": [
        "---\n",
        "\n",
        "# Step 11: Test GGUF Model\n",
        "\n",
        "Let's verify the quantized model works correctly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "c60KrsYiOEu-"
      },
      "outputs": [],
      "source": [
        "# Install llama-cpp-python for testing\n",
        "%%capture\n",
        "!pip install llama-cpp-python\n",
        "\n",
        "print(\"‚úÖ llama-cpp-python installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "WTNWRrcQOEu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254f4818-cdce-4799-e4b7-ce1a42445cb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading GGUF model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GGUF model loaded!\n",
            "   Model size: 0.81 GB\n",
            "   Context window: 512 tokens\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Load GGUF model\n",
        "print(\"üîÑ Loading GGUF model...\")\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=\"venvy_gemma3_q4km.gguf\",\n",
        "    n_ctx=512,  # Context window\n",
        "    n_threads=4,  # CPU threads\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ GGUF model loaded!\")\n",
        "print(f\"   Model size: {q4km_size:.2f} GB\")\n",
        "print(f\"   Context window: 512 tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "A5ieFSqCOEu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaddf3d9-7c01-4806-de00-917dfc1e2e66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing GGUF model...\n",
            "\n",
            "================================================================================\n",
            "Query: list all environments\n",
            "Response: COMMAND: venvy ls\n",
            "CONFIDENCE: 0.94\n",
            "EXPLANATION: Lists all registered environments\n",
            "--------------------------------------------------------------------------------\n",
            "Query: register this venv as myproject\n",
            "Response: COMMAND: venvy register --name myproject\n",
            "CONFIDENCE: 0.94\n",
            "EXPLANATION: Registers .venv with custom name 'myproject'\n",
            "--------------------------------------------------------------------------------\n",
            "Query: show current environment\n",
            "Response: COMMAND: venvy current\n",
            "CONFIDENCE: 0.93\n",
            "EXPLANATION: Shows currently active virtual environment\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "üí° If the responses look correct, your GGUF model is ready!\n"
          ]
        }
      ],
      "source": [
        "# Test GGUF model on queries\n",
        "def test_gguf_translation(nl_query):\n",
        "    instruction = f\"Translate to venvy command: {nl_query}\"\n",
        "    prompt = alpaca_prompt.format(instruction, \"\")\n",
        "\n",
        "    response = llm(\n",
        "        prompt,\n",
        "        max_tokens=128,\n",
        "        temperature=0.1,\n",
        "        top_p=0.9,\n",
        "        stop=[\"<end_of_turn>\", \"\\n\\n\"],\n",
        "    )\n",
        "\n",
        "    return response['choices'][0]['text'].strip()\n",
        "\n",
        "print(\"üß™ Testing GGUF model...\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "test_queries = [\n",
        "    \"list all environments\",\n",
        "    \"register this venv as myproject\",\n",
        "    \"show current environment\",\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    response = test_gguf_translation(query)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "print(\"\\nüí° If the responses look correct, your GGUF model is ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U685-Y-SOEu_"
      },
      "source": [
        "---\n",
        "\n",
        "# Step 12: Download the Models\n",
        "\n",
        "Download these files to your local machine:\n",
        "1. `venvy_gemma3_q4km.gguf` - Final quantized model (~600MB)\n",
        "2. `venvy_gemma3_lora/` - LoRA adapters (~16MB)\n",
        "\n",
        "You can also push them to your GitHub repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "JoXPKwUsOEu_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "c74955f7-cd29-4ff5-a385-387451f5d8e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Downloading models...\n",
            "   This may take a few minutes for the 600MB GGUF file\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8998ebcb-0ca8-4503-80a8-8a03082609e3\", \"venvy_gemma3_q4km.gguf\", 806057856)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Model downloaded!\n",
            "\n",
            "üí° To push to GitHub:\n",
            "   1. Create models/ directory in your repo\n",
            "   2. Add venvy_gemma3_q4km.gguf to models/\n",
            "   3. Use Git LFS for large files (>100MB)\n",
            "   4. Or host on HuggingFace Model Hub\n"
          ]
        }
      ],
      "source": [
        "# Option 1: Download via Colab UI\n",
        "from google.colab import files\n",
        "\n",
        "print(\"üì• Downloading models...\")\n",
        "print(\"   This may take a few minutes for the 600MB GGUF file\\n\")\n",
        "\n",
        "# Download GGUF model\n",
        "files.download('venvy_gemma3_q4km.gguf')\n",
        "\n",
        "print(\"\\n‚úÖ Model downloaded!\")\n",
        "print(\"\\nüí° To push to GitHub:\")\n",
        "print(\"   1. Create models/ directory in your repo\")\n",
        "print(\"   2. Add venvy_gemma3_q4km.gguf to models/\")\n",
        "print(\"   3. Use Git LFS for large files (>100MB)\")\n",
        "print(\"   4. Or host on HuggingFace Model Hub\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZJoJNBhOEu_"
      },
      "source": [
        "---\n",
        "\n",
        "# üéâ Training Complete!\n",
        "\n",
        "## What You Accomplished:\n",
        "\n",
        "1. ‚úÖ **Fine-tuned Gemma 3 1B** on 1,350 venvy command examples\n",
        "2. ‚úÖ **Used QLoRA** for efficient training (99.3% parameter reduction)\n",
        "3. ‚úÖ **Leveraged Unsloth** for 2x speed, 70% less VRAM\n",
        "4. ‚úÖ **Quantized to Q4_K_M** with importance matrix\n",
        "5. ‚úÖ **Created GGUF model** for CPU inference (~600MB)\n",
        "\n",
        "## What You Learned:\n",
        "\n",
        "### 1. Unsloth Benefits:\n",
        "- Custom CUDA kernels for 2x faster LoRA operations\n",
        "- Dynamic 4-bit quantization (smart layer selection)\n",
        "- Gradient checkpointing for 70% VRAM reduction\n",
        "\n",
        "### 2. QLoRA Mechanics:\n",
        "- Low-rank decomposition (A √ó B instead of full W)\n",
        "- Train only 0.7% of parameters (8M vs 1.1B)\n",
        "- NF4 quantization for base model (4-bit compressed)\n",
        "\n",
        "### 3. Quantization Techniques:\n",
        "- **4-bit Quantization**: 4x compression with minimal loss\n",
        "- **K-means Clustering**: Optimized bins for weight distribution\n",
        "- **Importance Matrix**: Preserve critical layers\n",
        "- **GGUF Format**: Optimized for CPU inference\n",
        "\n",
        "### 4. Training Best Practices:\n",
        "- Learning rate warmup prevents early instability\n",
        "- Cosine decay improves final convergence\n",
        "- Gradient accumulation enables larger effective batch size\n",
        "- Mixed precision (FP16) doubles speed on modern GPUs\n",
        "\n",
        "## Next Steps:\n",
        "\n",
        "1. **Integrate with venvy** - Add NL parser using llama-cpp-python\n",
        "2. **Test accuracy** - Evaluate on held-out examples\n",
        "3. **Optimize inference** - Add caching, daemon process\n",
        "4. **Create demo** - Video showing natural language CLI\n",
        "\n",
        "## Files to Keep:\n",
        "\n",
        "```\n",
        "venvy_gemma3_q4km.gguf        # Final model (~600MB) ‚úÖ IMPORTANT\n",
        "venvy_gemma3_lora/            # LoRA adapters (~16MB)\n",
        "venvy_imatrix.dat             # Importance matrix\n",
        "training_logs.txt             # Training metrics\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations! You've successfully fine-tuned a state-of-the-art SLM! üéä**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BV2ExsrEnrqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YZkK-ES1nrn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zVCy9hNXnrks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fel7VVx4nriU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test.py\n",
        "\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Load model\n",
        "llm = Llama(\n",
        "    model_path=\"models/venvy_gemma3_q4km.gguf\",\n",
        "    n_ctx=512,\n",
        "    n_threads=4,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "def translate(nl_query):\n",
        "    \"\"\"Translate natural language to venvy command.\"\"\"\n",
        "    prompt = f\"\"\"<start_of_turn>user\n",
        "Translate to venvy command: {nl_query}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "    response = llm(prompt, max_tokens=128, temperature=0.1, stop=[\"<end_of_turn>\"])\n",
        "    return response['choices'][0]['text'].strip()\n",
        "\n",
        "# Demo\n",
        "queries = [\n",
        "    \"list all environments\",\n",
        "    \"register this venv as myproject\",\n",
        "    \"show current environment\",\n",
        "    \"cleanup old venvs\",\n",
        "    \"scan home directory for environments\",\n",
        "    \"show statistics\"\n",
        "]\n",
        "\n",
        "print(\"ü§ñ Gemma 3 1B - venvy Command Translator\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for query in queries:\n",
        "    result = translate(query)\n",
        "    print(f\"\\nüí¨ Query: \\\"{query}\\\"\")\n",
        "    print(f\"‚ö° Output:\")\n",
        "    print(result)\n",
        "    print(\"-\"*80)"
      ],
      "metadata": {
        "id": "YBmSGgNcdLFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate_accuracy.py\n",
        "from llama_cpp import Llama\n",
        "import json\n",
        "\n",
        "llm = Llama(model_path=\"models/venvy_gemma3_q4km.gguf\", n_ctx=512, n_threads=4)\n",
        "\n",
        "# Load validation set\n",
        "with open('data/venvy_training.jsonl') as f:\n",
        "    examples = [json.loads(line) for line in f][-150:]  # Last 150 = validation\n",
        "\n",
        "correct = 0\n",
        "total = len(examples)\n",
        "\n",
        "for ex in examples:\n",
        "    query = ex['instruction'].replace('Translate to venvy command: ', '')\n",
        "    expected_cmd = ex['output'].split('COMMAND: ')[1].split('\\n')[0]\n",
        "\n",
        "    # Get model prediction\n",
        "    prompt = f\"<start_of_turn>user\\n{ex['instruction']}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    response = llm(prompt, max_tokens=128, temperature=0.1, stop=[\"<end_of_turn>\"])\n",
        "    predicted = response['choices'][0]['text'].strip()\n",
        "\n",
        "    # Extract command\n",
        "    if 'COMMAND:' in predicted:\n",
        "        predicted_cmd = predicted.split('COMMAND: ')[1].split('\\n')[0].strip()\n",
        "    else:\n",
        "        predicted_cmd = predicted.split('\\n')[0].strip()\n",
        "\n",
        "    # Check if correct\n",
        "    if predicted_cmd == expected_cmd:\n",
        "        correct += 1\n",
        "    else:\n",
        "        print(f\"‚ùå Query: {query}\")\n",
        "        print(f\"   Expected: {expected_cmd}\")\n",
        "        print(f\"   Got: {predicted_cmd}\\n\")\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"\\nüìä Accuracy: {correct}/{total} = {accuracy:.1%}\")"
      ],
      "metadata": {
        "id": "LzjNsY4XdLBM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "58c0b00caae74705b4cec63a68057176": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6aa55c1416240c19abc8a036f530eb4",
              "IPY_MODEL_31f725f224114ac4a6a386ac15cdd455",
              "IPY_MODEL_f982825f9251462b98bb8b7549eb745a"
            ],
            "layout": "IPY_MODEL_087d014f07ca478888687a341625c893"
          }
        },
        "f6aa55c1416240c19abc8a036f530eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09de1be93d3647faafc777fc7a8c82a3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9e347f85d56d43b3a31863e832f967c1",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "31f725f224114ac4a6a386ac15cdd455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6354ea3438c4f85947a3f098ff269b8",
            "max": 1000024720,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee082d176a5f4d099842c3d0c033828c",
            "value": 1000024720
          }
        },
        "f982825f9251462b98bb8b7549eb745a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02dea77f533f4e1fb56876f61f4d4009",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d27997e3675d40efb024c056cda6d79e",
            "value": "‚Äá1.00G/1.00G‚Äá[00:09&lt;00:00,‚Äá151MB/s]"
          }
        },
        "087d014f07ca478888687a341625c893": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09de1be93d3647faafc777fc7a8c82a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e347f85d56d43b3a31863e832f967c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6354ea3438c4f85947a3f098ff269b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee082d176a5f4d099842c3d0c033828c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "02dea77f533f4e1fb56876f61f4d4009": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d27997e3675d40efb024c056cda6d79e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0329d70c15134b19979a07d0ec26a65a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7bcdfd3449544c928baad9d00b5a5a08",
              "IPY_MODEL_d28a133d0acf43fba1d2ebe833751865",
              "IPY_MODEL_b96eb8224548459c88019a771ed0b7ad"
            ],
            "layout": "IPY_MODEL_ea4f1a76bbf148818848cd9fb08443f7"
          }
        },
        "7bcdfd3449544c928baad9d00b5a5a08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f724e469582d4195923d33b2847e3632",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_951b7ed0401e490295f05e40e16ce500",
            "value": "generation_config.json:‚Äá100%"
          }
        },
        "d28a133d0acf43fba1d2ebe833751865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fe16f70bc2d4101ab7fc5be9d7c1528",
            "max": 233,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdb0cd8b8e024997a8d00058b30ac5bb",
            "value": 233
          }
        },
        "b96eb8224548459c88019a771ed0b7ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bafc3c9850084c91ba20af59f588acc9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_acc001a0d08c4fcfb8bc2adfb0a64e6b",
            "value": "‚Äá233/233‚Äá[00:00&lt;00:00,‚Äá25.6kB/s]"
          }
        },
        "ea4f1a76bbf148818848cd9fb08443f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f724e469582d4195923d33b2847e3632": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "951b7ed0401e490295f05e40e16ce500": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fe16f70bc2d4101ab7fc5be9d7c1528": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdb0cd8b8e024997a8d00058b30ac5bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bafc3c9850084c91ba20af59f588acc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acc001a0d08c4fcfb8bc2adfb0a64e6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93821e90be744ce28ddfb8de178d3a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0eaa29f67e9460ba2bb2caa931d7013",
              "IPY_MODEL_3cc019342cca4e1da664977411a45d81",
              "IPY_MODEL_ed17f1a1dfbd420fbd8d5a4ddbe29490"
            ],
            "layout": "IPY_MODEL_825d9b69c30240819c51cd0a5b996d79"
          }
        },
        "c0eaa29f67e9460ba2bb2caa931d7013": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32c67b09a77c4476853063ca928b546f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e9c6e9d3470843a48b7a9200aae617b1",
            "value": "tokenizer_config.json:‚Äá"
          }
        },
        "3cc019342cca4e1da664977411a45d81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e55d34c2bac4d9f8ea28129cc5cdb69",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4e24dfe333f46fcab5b451c5ca2c566",
            "value": 1
          }
        },
        "ed17f1a1dfbd420fbd8d5a4ddbe29490": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9787a57a776b46ddb0a99694e8385a6a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6b9d5ba50fd54d95a5ee1ace2ad460ba",
            "value": "‚Äá1.16M/?‚Äá[00:00&lt;00:00,‚Äá64.3MB/s]"
          }
        },
        "825d9b69c30240819c51cd0a5b996d79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32c67b09a77c4476853063ca928b546f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9c6e9d3470843a48b7a9200aae617b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e55d34c2bac4d9f8ea28129cc5cdb69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d4e24dfe333f46fcab5b451c5ca2c566": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9787a57a776b46ddb0a99694e8385a6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b9d5ba50fd54d95a5ee1ace2ad460ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f68ebd74ff2b462f89814deb09b096f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6160865ba044dc98d6d37de5a81622f",
              "IPY_MODEL_d81cd324c368470b9d8f729fcd1ae711",
              "IPY_MODEL_e7002e44893f4ab390e38101bd2aab4a"
            ],
            "layout": "IPY_MODEL_53cf547e1f7c4d0fb56756871261e875"
          }
        },
        "e6160865ba044dc98d6d37de5a81622f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51fceabbb510478ab1cd2b0eb6bc107b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1efef71ffb344721a138ed9b78ac426f",
            "value": "tokenizer.model:‚Äá100%"
          }
        },
        "d81cd324c368470b9d8f729fcd1ae711": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_233c60cfca7c441eabcc440ea290b3db",
            "max": 4689074,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52c087f93f254fc5abe94958b4018762",
            "value": 4689074
          }
        },
        "e7002e44893f4ab390e38101bd2aab4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2decb2e6d39043179cee49f6358bef7b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_64b3be1d0fc146848243c69ae1c8bc48",
            "value": "‚Äá4.69M/4.69M‚Äá[00:00&lt;00:00,‚Äá7.35MB/s]"
          }
        },
        "53cf547e1f7c4d0fb56756871261e875": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51fceabbb510478ab1cd2b0eb6bc107b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1efef71ffb344721a138ed9b78ac426f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "233c60cfca7c441eabcc440ea290b3db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52c087f93f254fc5abe94958b4018762": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2decb2e6d39043179cee49f6358bef7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64b3be1d0fc146848243c69ae1c8bc48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4516c29e8acf4f08a6a1fade33ab78aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ddd2681a2bac486397110a3f0284cc41",
              "IPY_MODEL_471f236f23834677acd3a4fa65b704f7",
              "IPY_MODEL_328793a850234612a97450618e380db9"
            ],
            "layout": "IPY_MODEL_44035936bb50474d958a417f32405a74"
          }
        },
        "ddd2681a2bac486397110a3f0284cc41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a82344ace7b84f4fb2700f8e1f568caa",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_894a0ca53c794a528773378098b25e05",
            "value": "tokenizer.json:‚Äá100%"
          }
        },
        "471f236f23834677acd3a4fa65b704f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4245fc8a5a9543d88f62cd489ddd36f9",
            "max": 33384568,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e2eefab6ec14718aefc79b999e19afd",
            "value": 33384568
          }
        },
        "328793a850234612a97450618e380db9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b9deff0cec74a129d7fd5dbb3be4016",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ea948544ce3b4d1b8bc5d7c6729fd691",
            "value": "‚Äá33.4M/33.4M‚Äá[00:00&lt;00:00,‚Äá58.2MB/s]"
          }
        },
        "44035936bb50474d958a417f32405a74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a82344ace7b84f4fb2700f8e1f568caa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "894a0ca53c794a528773378098b25e05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4245fc8a5a9543d88f62cd489ddd36f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e2eefab6ec14718aefc79b999e19afd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b9deff0cec74a129d7fd5dbb3be4016": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea948544ce3b4d1b8bc5d7c6729fd691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d871e02c835b41a6818201d29ac39519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_848b3a570f15463bb1a5d8aaf63a94e1",
              "IPY_MODEL_505fb0c10c8149729b851208b74d5fb9",
              "IPY_MODEL_317bcd6724ff460aa673fd9ed1309069"
            ],
            "layout": "IPY_MODEL_d495504d94dc4c7abb680afa1dedca20"
          }
        },
        "848b3a570f15463bb1a5d8aaf63a94e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab4f1ae9a8d9465baf0d1b972a061242",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a58fd094c3cf4b9aa6adbb6af2741daf",
            "value": "added_tokens.json:‚Äá100%"
          }
        },
        "505fb0c10c8149729b851208b74d5fb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32351141b5cf4b00aeb348651db061a4",
            "max": 35,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad31a8ed6940464489246dac083dc1fd",
            "value": 35
          }
        },
        "317bcd6724ff460aa673fd9ed1309069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c5fd844742b4ae18467e58a78acddb9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_28c8e954be7148c6abe3a80b1698e693",
            "value": "‚Äá35.0/35.0‚Äá[00:00&lt;00:00,‚Äá3.67kB/s]"
          }
        },
        "d495504d94dc4c7abb680afa1dedca20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab4f1ae9a8d9465baf0d1b972a061242": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a58fd094c3cf4b9aa6adbb6af2741daf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32351141b5cf4b00aeb348651db061a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad31a8ed6940464489246dac083dc1fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c5fd844742b4ae18467e58a78acddb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28c8e954be7148c6abe3a80b1698e693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "612260e6f8644210ae4e0795c8b37809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ec7d8cafa4445bebea57b7bd75ad916",
              "IPY_MODEL_e8ea2a14d69b43d2aada34e0385a8e36",
              "IPY_MODEL_8e5c9726f79546d282419dd4e90c255e"
            ],
            "layout": "IPY_MODEL_9dc1696e897d4e1e8fe5b41824ba6286"
          }
        },
        "3ec7d8cafa4445bebea57b7bd75ad916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87bca939e938497fa7f2bd8eac3817ee",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1d5209a61b4045cea059103dbeb5a93a",
            "value": "special_tokens_map.json:‚Äá100%"
          }
        },
        "e8ea2a14d69b43d2aada34e0385a8e36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cc8e9daa6394d6a905ce7d4d56236cd",
            "max": 670,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ecf6380592c64d6399e509871c0f398b",
            "value": 670
          }
        },
        "8e5c9726f79546d282419dd4e90c255e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26e1669317cc46d1b2f3a698b8be9652",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_82fe3cb16b934683a302061b28d49732",
            "value": "‚Äá670/670‚Äá[00:00&lt;00:00,‚Äá69.0kB/s]"
          }
        },
        "9dc1696e897d4e1e8fe5b41824ba6286": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87bca939e938497fa7f2bd8eac3817ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d5209a61b4045cea059103dbeb5a93a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cc8e9daa6394d6a905ce7d4d56236cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecf6380592c64d6399e509871c0f398b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "26e1669317cc46d1b2f3a698b8be9652": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82fe3cb16b934683a302061b28d49732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "280ca4a361124b029058eb9266360eae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b91f1c17e2b4e518e1ef00dd90f76eb",
              "IPY_MODEL_75d899987b844a3fa06c23d464f92a41",
              "IPY_MODEL_5bb0fa44a8ce436e9f318dedca2aca42"
            ],
            "layout": "IPY_MODEL_36c5153b9caf4bafa5783786fc5510ad"
          }
        },
        "1b91f1c17e2b4e518e1ef00dd90f76eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48f36b53e1f04014b03b0b1286a56817",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_243c2aefb0f8410a8cb576ad0494525a",
            "value": "Generating‚Äátrain‚Äásplit:‚Äá"
          }
        },
        "75d899987b844a3fa06c23d464f92a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1a5fc3155a34dadabe48106233772db",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_541599a1cd1241a7bcb3332ed5966d00",
            "value": 1
          }
        },
        "5bb0fa44a8ce436e9f318dedca2aca42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbd44b2eee934c60b4390cee1534292e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_27753014a8ff42e1baa263dd96220f66",
            "value": "‚Äá1500/0‚Äá[00:00&lt;00:00,‚Äá19542.57‚Äáexamples/s]"
          }
        },
        "36c5153b9caf4bafa5783786fc5510ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48f36b53e1f04014b03b0b1286a56817": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "243c2aefb0f8410a8cb576ad0494525a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1a5fc3155a34dadabe48106233772db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "541599a1cd1241a7bcb3332ed5966d00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dbd44b2eee934c60b4390cee1534292e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27753014a8ff42e1baa263dd96220f66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "711faff2256e4788893512683ab1203f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab06957e76d14f0abece120932878cde",
              "IPY_MODEL_d1743f715a8b426aa13370d360943a83",
              "IPY_MODEL_0442381ee1014781b7025af4dff4b5ae"
            ],
            "layout": "IPY_MODEL_805e4a5d1acd4447aaaa793dd076cde6"
          }
        },
        "ab06957e76d14f0abece120932878cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47ac8a4565e542ba8f57aeb631078037",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1e69dfdc92a647f1865d2c8f9683e892",
            "value": "Map:‚Äá100%"
          }
        },
        "d1743f715a8b426aa13370d360943a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97d7f2b08cb140c2800fc1d7f66ec4d1",
            "max": 1350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ceb3907b37134d9c98f4ecb208d1cb14",
            "value": 1350
          }
        },
        "0442381ee1014781b7025af4dff4b5ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a274683b5ecf42a2b453a46c2a934a28",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f2062dcfe1314122ba847fe2b4e762c6",
            "value": "‚Äá1350/1350‚Äá[00:00&lt;00:00,‚Äá15638.63‚Äáexamples/s]"
          }
        },
        "805e4a5d1acd4447aaaa793dd076cde6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47ac8a4565e542ba8f57aeb631078037": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e69dfdc92a647f1865d2c8f9683e892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97d7f2b08cb140c2800fc1d7f66ec4d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceb3907b37134d9c98f4ecb208d1cb14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a274683b5ecf42a2b453a46c2a934a28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2062dcfe1314122ba847fe2b4e762c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ef6f2edaeb34d6784d17b8df1006372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c99a956b7ed41d8aaf3f399bafa111b",
              "IPY_MODEL_a0eb193e1f9f4ada9205e3391722dbbf",
              "IPY_MODEL_c9e73ea2f1504ba2b2d7169abbb6ec62"
            ],
            "layout": "IPY_MODEL_c480611d7d0f42c685a7bfe60c1c61b7"
          }
        },
        "2c99a956b7ed41d8aaf3f399bafa111b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12a2f2b57ae94e3dba6e558184f049ed",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ecb18210a2ba4972900e58a410c9646b",
            "value": "Map:‚Äá100%"
          }
        },
        "a0eb193e1f9f4ada9205e3391722dbbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8302d51430664f3ea095dd2ee30c8a90",
            "max": 150,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4e06b319ea84734abc4690e61fa0b4e",
            "value": 150
          }
        },
        "c9e73ea2f1504ba2b2d7169abbb6ec62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9e74ba984e44ab09f680c5cf5e1d33c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_65939adc0f9645aca7400bcfcae5eabb",
            "value": "‚Äá150/150‚Äá[00:00&lt;00:00,‚Äá2490.48‚Äáexamples/s]"
          }
        },
        "c480611d7d0f42c685a7bfe60c1c61b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12a2f2b57ae94e3dba6e558184f049ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecb18210a2ba4972900e58a410c9646b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8302d51430664f3ea095dd2ee30c8a90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4e06b319ea84734abc4690e61fa0b4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9e74ba984e44ab09f680c5cf5e1d33c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65939adc0f9645aca7400bcfcae5eabb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bf7332fda8f42ffa2dfbdf8a90cc398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58fad93163194d91acfb3540840b8502",
              "IPY_MODEL_c4dc028e95a24d3a87d44319cf6e0da0",
              "IPY_MODEL_a8cbe9bc4bab4f9e8a5d1f6096399b27"
            ],
            "layout": "IPY_MODEL_3732151639fa4d739b12121e48b3d14b"
          }
        },
        "58fad93163194d91acfb3540840b8502": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8610d57adf646cdbf954eec3a0ae7ed",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_023e99de1a75494bb7491631dcf296fc",
            "value": "Unsloth:‚ÄáTokenizing‚Äá[&quot;text&quot;]‚Äá(num_proc=6):‚Äá100%"
          }
        },
        "c4dc028e95a24d3a87d44319cf6e0da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d48cd0fd7d64f29ac45f51f785a8078",
            "max": 1350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27097ab156c54b6bb0003be7e4f11d22",
            "value": 1350
          }
        },
        "a8cbe9bc4bab4f9e8a5d1f6096399b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24eb81674cb04959b499814b09178964",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5ec221d57968463e9fbb7cec0c43d16e",
            "value": "‚Äá1350/1350‚Äá[00:16&lt;00:00,‚Äá139.13‚Äáexamples/s]"
          }
        },
        "3732151639fa4d739b12121e48b3d14b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8610d57adf646cdbf954eec3a0ae7ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "023e99de1a75494bb7491631dcf296fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d48cd0fd7d64f29ac45f51f785a8078": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27097ab156c54b6bb0003be7e4f11d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24eb81674cb04959b499814b09178964": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ec221d57968463e9fbb7cec0c43d16e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea42129706cb4083acc5a77d5e705599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e3b486b413544f8a2a5f074e3bd9acc",
              "IPY_MODEL_7c3e7736a9174855807a7e17c501a2c8",
              "IPY_MODEL_6b07d7ceeb1440a290829f7b9ae4ff92"
            ],
            "layout": "IPY_MODEL_4c42672cedf94310ad65a93f5daf5cd1"
          }
        },
        "4e3b486b413544f8a2a5f074e3bd9acc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93f57d2217fb4b0bb1b67e4f31b9be87",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_20cd791d40814a49bd6e72efad1baee5",
            "value": "Unsloth:‚ÄáTokenizing‚Äá[&quot;text&quot;]‚Äá(num_proc=6):‚Äá100%"
          }
        },
        "7c3e7736a9174855807a7e17c501a2c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38e39bbb997d4069b4b240963d971eb1",
            "max": 150,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25470b84989f44ecb2939c024fa9cae4",
            "value": 150
          }
        },
        "6b07d7ceeb1440a290829f7b9ae4ff92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9eff16482fc47c9997f878168423f75",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e45d94832b1f4b82bb25a7ffe8d9e9df",
            "value": "‚Äá150/150‚Äá[00:18&lt;00:00,‚Äá13.78‚Äáexamples/s]"
          }
        },
        "4c42672cedf94310ad65a93f5daf5cd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93f57d2217fb4b0bb1b67e4f31b9be87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20cd791d40814a49bd6e72efad1baee5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38e39bbb997d4069b4b240963d971eb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25470b84989f44ecb2939c024fa9cae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9eff16482fc47c9997f878168423f75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e45d94832b1f4b82bb25a7ffe8d9e9df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d72a0f96e4364957b094fd4c48b0e578": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2b85d77d18c44f8b01cb55db28fd511",
              "IPY_MODEL_9f694044be6241539544d3502075e4b3",
              "IPY_MODEL_95227a5decfc48799037dbd2131de79f"
            ],
            "layout": "IPY_MODEL_debd13db8fa7403abcc295c78475e5ff"
          }
        },
        "c2b85d77d18c44f8b01cb55db28fd511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5851d15c8018425d86e9f5911a510be6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_681bec1714b94d1d8a4c7cd6f8571fe1",
            "value": "config.json:‚Äá100%"
          }
        },
        "9f694044be6241539544d3502075e4b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97b6862126644116850c14e12967da53",
            "max": 902,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe10c4b83a2d45819f2deb99e3c4875b",
            "value": 902
          }
        },
        "95227a5decfc48799037dbd2131de79f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af94e6f0cb7e45bc988b0b107053fb50",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7d80c3b7dfda411980f773523e639244",
            "value": "‚Äá902/902‚Äá[00:00&lt;00:00,‚Äá90.8kB/s]"
          }
        },
        "debd13db8fa7403abcc295c78475e5ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5851d15c8018425d86e9f5911a510be6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "681bec1714b94d1d8a4c7cd6f8571fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97b6862126644116850c14e12967da53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe10c4b83a2d45819f2deb99e3c4875b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af94e6f0cb7e45bc988b0b107053fb50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d80c3b7dfda411980f773523e639244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf5fd14d65de4274b0fbcfcc863a4e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db07dfe1f6624f40a0e8f9be630cf0f5",
              "IPY_MODEL_0d7e893bee83462f8c080e2e13463db4",
              "IPY_MODEL_906f8f3cc82b408a96e96e39852207b9"
            ],
            "layout": "IPY_MODEL_71f3de21b99e49d38c5ffaae8ee5fc7f"
          }
        },
        "db07dfe1f6624f40a0e8f9be630cf0f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a13ace312304a7385e5bfe8d20ab1f2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ba4dd531ab14463e830c4bc22fd46c5e",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "0d7e893bee83462f8c080e2e13463db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d912479026f04fa88fb62c113eabb579",
            "max": 1999811208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b44cb30658c4519b965709a585309ea",
            "value": 1999811208
          }
        },
        "906f8f3cc82b408a96e96e39852207b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5850fcaff27f4a658dfa0024574cecec",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5ba3a7ff06c94c3b9037d7be02271186",
            "value": "‚Äá2.00G/2.00G‚Äá[00:17&lt;00:00,‚Äá78.6MB/s]"
          }
        },
        "71f3de21b99e49d38c5ffaae8ee5fc7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a13ace312304a7385e5bfe8d20ab1f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba4dd531ab14463e830c4bc22fd46c5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d912479026f04fa88fb62c113eabb579": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b44cb30658c4519b965709a585309ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5850fcaff27f4a658dfa0024574cecec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ba3a7ff06c94c3b9037d7be02271186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}